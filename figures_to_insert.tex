%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% HYALINE MANUSCRIPT FIGURES
% 
% This file contains all figures for the Hyaline manuscript,
% formatted according to Nature Communications standards.
% 
% USAGE: Replace the placeholder \fbox{\parbox{...}} blocks in
%        hyaline_manuscript.tex with these figure definitions.
%
% Each figure includes:
%   - Proper \includegraphics command
%   - Nature Communications-level caption
%   - Correct label matching existing references
%
% Required figure files (place in same directory as .tex or figures/):
%   - global_diagram.pdf
%   - benchmark_comparison.pdf  (or create from benchmark bar chart)
%   - egnn_layer.pdf
%   - motif_attention_analysis.pdf (or create from motif attention bar chart)
%   - ablation_analysis.pdf (or create from ablation bar chart)
%   - 2RH1_dry.png
%   - 3SN6_dry.png
%   - grphread_classif.pdf
%   - scalability_analysis.pdf (or create from scalability line plot)
%   - latent_space_tsne.pdf (or create from t-SNE plot)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%=======================================================================
% MAIN TEXT FIGURES (1-6)
%=======================================================================

%-----------------------------------------------------------------------
% FIGURE 1: The Hyaline Framework
% Asset: global_diagram.pdf
% Placement: Section 2.1 - Top of Results, immediately after intro paragraph
%-----------------------------------------------------------------------
\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{global_diagram.pdf}
\caption{\textbf{The Hyaline geometric deep learning framework.} 
The architecture integrates evolutionary sequence information with three-dimensional structural geometry to predict \gpcr{} activation states. 
\textbf{a}, The pipeline processes amino acid sequences using the ESM3 protein language model, generating 1,536-dimensional per-residue embeddings that encode evolutionary constraints derived from billions of protein sequences. In parallel, structural geometry is encoded via radial basis function (RBF) radius graphs constructed with a 10~\AA{} cutoff, producing 96-dimensional distance features for each edge. 
\textbf{b}, These complementary modalities---evolutionary embeddings and geometric features---are fused within a stack of five E(n)-equivariant message-passing layers. This equivariant formulation ensures that predictions depend solely on relative atomic arrangements, maintaining strict invariance to rotations and translations of the input coordinate frame. 
\textbf{c}, Biological priors are injected through a soft motif-attention biasing mechanism that guides the network to prioritize residues within conserved activation microswitches (DRY, NPxxY, CWxP regions) while preserving full representational flexibility to discover novel structural determinants from training data. Global attention pooling aggregates per-residue features into a graph-level representation for final binary classification.}
\label{fig:architecture}
\end{figure*}


%-----------------------------------------------------------------------
% FIGURE 2: Temporal Generalization Benchmark
% Asset: benchmark_comparison.pdf (bar chart comparing Hyaline vs baselines)
% Placement: Section 2.2 - "Hyaline generalizes to temporally distinct structures"
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{benchmark_comparison.pdf}
\caption{\textbf{Hyaline generalizes to temporally distinct structures.} 
Performance evaluation on a held-out test set comprising structures deposited in 2023--2024, strictly separated from the pre-2023 training corpus. \hyaline{} (purple) achieves an \auroc{} of 0.991, significantly outperforming sequence-only baselines including ESM3 embeddings with linear classifier (\auroc{} = 0.852), ESM3 with MLP (\auroc{} = 0.867), and random forest models trained on hand-crafted inter-residue distance features (\auroc{} = 0.891). The 13.9 percentage point performance gap relative to the ESM3-only baseline demonstrates the critical contribution of geometric message passing in resolving conformational states that remain indistinguishable by amino acid sequence information alone. Error bars denote 95\% confidence intervals computed via bootstrap resampling ($n = 1{,}000$).}
\label{fig:performance}
\end{figure}


%-----------------------------------------------------------------------
% FIGURE 3: The Geometric Engine (Mechanism)
% Asset: egnn_layer.pdf
% Placement: Section 2.3 - "Attention mechanisms..."
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{egnn_layer.pdf}
\caption{\textbf{Enhanced E(n)-equivariant message passing with biological priors.} 
Detailed schematic of a single \hyaline{} layer illustrating the dual update mechanism. The architecture simultaneously updates node coordinates $x_i$ and feature vectors $h_i$ through message functions $\phi_e$ that depend on pairwise distances and RBF-encoded edge attributes, thereby maintaining strict rotational equivariance by construction. The Motif Bias block (highlighted in red) injects learnable attention weights directly into the message aggregation mechanism, providing an inductive bias that guides the network to prioritize residues within conserved microswitches---the ionic lock formed by the DRY motif (Asp-Arg-Tyr in TM3), the NPxxY rotamer toggle (TM7), and the CWxP transmission switch (TM6)---while retaining full capacity to learn novel geometric features from the training distribution. This soft biasing accelerates convergence and improves robustness to distribution shift without hard-coding classification rules.}
\label{fig:egnn_mechanism}
\end{figure}


%-----------------------------------------------------------------------
% FIGURE 4: Interpretability & Motif Analysis
% Asset: motif_attention_analysis.pdf (bar chart of DRY/NPxxY/CWxP weights)
% Placement: Section 2.3 - Following Figure 3
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{motif_attention_analysis.pdf}
\caption{\textbf{Autonomous recovery of conserved activation switches.} 
Quantitative analysis of learned attention weights across correctly classified structures in the temporal test set reveals that \hyaline{} spontaneously prioritizes functionally critical regions without explicit supervision. The model assigns significantly elevated importance to the three conserved activation motifs---DRY (3.2$\times$ enrichment relative to non-motif residues), NPxxY (2.8$\times$), and CWxP (2.4$\times$)---compared to variable regions such as the PIF motif ($p < 0.001$, permutation test with $n = 10{,}000$ resamples). Additional attention peaks at the intracellular end of TM5 (2.1$\times$) and helix 8 (1.9$\times$) correspond to established G protein-binding elements, further validating that the network has learned the underlying biophysics of receptor activation rather than superficial structural correlates or dataset-specific artifacts.}
\label{fig:interpretability}
\end{figure}


%-----------------------------------------------------------------------
% FIGURE 5: Ablation Studies
% Asset: ablation_analysis.pdf (bar chart showing component contributions)
% Placement: Section 2.5 - "Ablation studies..."
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{ablation_analysis.pdf}
\caption{\textbf{Deconstructing the value of geometric priors.} 
Systematic ablation studies quantify the independent contribution of each architectural component to classification performance. Removal of the geometric structure encoding (No RBF/GNN) produces the steepest performance decline ($\Delta$\auroc{} = $-$17.2\%), confirming that three-dimensional geometry constitutes the primary driver of accurate state discrimination---sequence-derived features alone, despite their richness, cannot resolve the conformational differences between active and inactive states. Ablating the motif-biased attention mechanism results in a more modest but statistically significant decrease ($\Delta$\auroc{} = $-$2.7\%, $p < 0.01$), validating the utility of domain-specific inductive biases in accelerating learning and improving generalization. The RBF distance encoding ($\Delta$\auroc{} = $-$2.4\%) and message-passing depth ($\Delta$\auroc{} = $-$1.4\% for 3 vs.\ 5 layers) each contribute incrementally, demonstrating that all components synergize to achieve optimal performance. All ablations evaluated via 5-fold cross-validation with cluster-based splitting at 30\% sequence identity.}
\label{fig:ablation}
\end{figure}


%-----------------------------------------------------------------------
% FIGURE 6: Structural Validation (The "Proof")
% Assets: 2RH1_dry.png and 3SN6_dry.png (side-by-side)
% Placement: End of Results / Section 2.6
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{2RH1_dry.png}
    \caption{Inactive state (PDB: 2RH1)}
    \label{fig:2rh1}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{3SN6_dry.png}
    \caption{Active state (PDB: 3SN6)}
    \label{fig:3sn6}
\end{subfigure}
\caption{\textbf{Structural basis of activation state discrimination.} 
\hyaline{} correctly identifies activation states by detecting fine-grained side-chain rearrangements at conserved microswitches, as illustrated for the prototypical $\beta_2$-adrenergic receptor. 
\textbf{a}, In the inactive conformation (PDB: 2RH1), the conserved DRY motif at the cytoplasmic end of TM3 maintains an intact ionic lock: the guanidinium group of Arg$^{3.50}$ (Ballesteros-Weinstein numbering) forms a salt bridge with Glu$^{6.30}$ on TM6 (distance: 3.4~\AA{}), stabilizing the closed intracellular conformation that occludes G protein binding. 
\textbf{b}, Upon activation by agonist and G protein coupling (PDB: 3SN6), this ionic lock is broken (R$^{3.50}$--E$^{6.30}$ distance: 11.2~\AA{}) as TM6 undergoes its characteristic 14~\AA{} outward displacement. Arg$^{3.50}$ rotates into the opened intracellular cavity where it coordinates the C-terminal helix of G$\alpha_s$. The elevated attention scores assigned by \hyaline{} to this region (Fig.~4) confirm that the model discriminates conformational states based on this precise physical mechanism rather than superficial structural correlates.}
\label{fig:structural_validation}
\end{figure}


%=======================================================================
% EXTENDED DATA FIGURES (Supplement)
%=======================================================================

%-----------------------------------------------------------------------
% EXTENDED DATA FIGURE 1: Readout Architecture
% Asset: grphread_classif.pdf
% Placement: Appendix/Extended Data
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{grphread_classif.pdf}
\caption{\textbf{Jumping Knowledge aggregation and readout architecture.} 
To mitigate over-smoothing---a common failure mode of deep graph neural networks where node representations become indistinguishable---and capture features at multiple spatial scales, node representations from all six layers (initial embedding plus five EGNN message-passing layers) are concatenated prior to global pooling. This Jumping Knowledge aggregation produces a 1,920-dimensional vector (320 $\times$ 6 layers) that is fused with the global graph state before projection through the final two-layer MLP classifier. This architecture ensures the readout captures both fine-grained local atomic environments from early layers (sensitive to immediate neighbor configurations) and integrated long-range receptor topology from deeper layers (sensitive to the global arrangement of transmembrane helices).}
\label{fig:ext_readout}
\end{figure}


%-----------------------------------------------------------------------
% EXTENDED DATA FIGURE 2: Computational Scalability
% Asset: scalability_analysis.pdf (line plot of inference time vs length)
% Placement: Appendix/Extended Data
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{scalability_analysis.pdf}
\caption{\textbf{Linear computational scaling enables high-throughput application.} 
Inference time as a function of protein sequence length demonstrates that \hyaline{}'s sparse radius graph implementation scales linearly with receptor size ($O(N)$ complexity), in contrast to dense pairwise attention mechanisms (e.g., Transformers without sparse attention) that scale quadratically ($O(N^2)$). This efficiency arises from the 10~\AA{} radius cutoff, which produces graphs with $O(N)$ edges rather than $O(N^2)$ for fully connected representations. The favorable scaling enables processing of typical \gpcr{} structures ($<$400 residues) in under 0.5 seconds on a single GPU (NVIDIA A100), facilitating high-throughput screening of AI-predicted structures, real-time assessment of molecular dynamics trajectories, and integration into iterative drug design workflows. Batch inference achieves throughput exceeding 7,200 structures per hour.}
\label{fig:ext_scalability}
\end{figure}


%-----------------------------------------------------------------------
% EXTENDED DATA FIGURE 3: Latent Space Analysis
% Asset: latent_space_tsne.pdf (t-SNE projection)
% Placement: Appendix/Extended Data
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{latent_space_tsne.pdf}
\caption{\textbf{Learned latent representations encode a universal activation axis.} 
t-SNE projection (perplexity = 30, 1,000 iterations) of the final hidden layer embeddings (320-dimensional graph representations after global pooling) for all structures in the temporal test set ($n = 278$). Structures cluster primarily by activation state (active versus inactive, indicated by color) rather than by sequence homology or receptor family, demonstrating that \hyaline{} has learned a generalizable ``activation axis'' that transfers across the mechanistically diverse Class A, B, C, and F receptor superfamilies. Secondary clustering by receptor class (indicated by marker shape) reflects family-specific structural features---such as the distinct TM6 kinking in Class B1 receptors versus the rigid-body TM6 rotation in Class A---that are superimposed on the shared activation signature. This hierarchical organization validates the model's ability to capture both universal physics of activation and receptor-specific structural nuances.}
\label{fig:ext_latent}
\end{figure}


%=======================================================================
% END OF FIGURES FILE
%=======================================================================