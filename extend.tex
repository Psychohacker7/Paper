%% =============================================================================
%% Hyaline: Geometric Deep Learning for GPCR Activation State Prediction
%% EXTENDED AND AUGMENTED MANUSCRIPT - Nature Communications Format
%% Comprehensive integration of GPCR activation mechanisms, AI methods, and
%% geometric deep learning concepts from current literature
%% =============================================================================

\documentclass[11pt,a4paper]{article}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{float}
\usepackage{authblk}
\usepackage{lineno}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{algorithmic}

% Line numbers for review
\linenumbers
\onehalfspacing

% Custom commands
\newcommand{\angstrom}{\text{\AA}}
\newcommand{\tm}[1]{TM#1}
\newcommand{\eg}{E(n)}
\newcommand{\auroc}{AuROC}
\newcommand{\hyaline}{\textsc{Hyaline}}
\newcommand{\bw}[1]{$^{#1}$}

\begin{document}

%% =============================================================================
%% TITLE
%% =============================================================================

\title{\textbf{Hyaline: E(n)-Equivariant Graph Neural Networks with\\
Protein Language Model Embeddings Enable Universal\\
G Protein-Coupled Receptor Activation State Prediction\\
Across Mechanistically Diverse Receptor Families}}

\author[1,*]{Author One}
\author[1]{Author Two}  
\author[2]{Author Three}
\author[1,2,$\dagger$]{Senior Author}

\affil[1]{Department of Computational Biology, University}
\affil[2]{Institute for Drug Discovery}
\affil[*]{Correspondence: author@university.edu}
\affil[$\dagger$]{Lead contact}

\date{}

\maketitle

%% =============================================================================
%% ABSTRACT
%% =============================================================================

\begin{abstract}

\noindent\textbf{Background:} G protein-coupled receptors (GPCRs) comprise the largest superfamily of membrane proteins, with over 800 members serving as targets for approximately one-third of FDA-approved therapeutics. The conformational transition between inactive and active states---characterized by conserved structural rearrangements including the universal outward movement of transmembrane helix 6 (TM6)---fundamentally determines receptor signaling and therapeutic response. However, the magnitude and mechanism of activation varies dramatically across GPCR classes: Class A receptors exhibit 7--13~\angstrom{} TM6 displacement with 38$^\circ$ rotation; Class B1 receptors display larger movements (14--19~\angstrom{}) combined with unique extracellular helix unwinding; Class C receptors operate through asymmetric dimer reorientation with minimal TM6 movement (2.8~\angstrom{}); and Class F receptors show intermediate mechanisms. This mechanistic diversity, combined with the limitations of existing computational approaches that rely on class-specific geometric descriptors, has precluded development of universal activation state classifiers.

\noindent\textbf{Methods:} We present \hyaline{} (Helix-based Yielding Algorithm for Learning Interpretable Neural Embeddings), a geometric deep learning framework integrating three key innovations: (1) evolutionary sequence embeddings from the ESM3 protein language model, which encodes structural and functional constraints learned from 15 billion protein sequences; (2) E(n)-equivariant graph neural networks that maintain rotational and translational invariance through geometric message passing on three-dimensional coordinates; and (3) biologically-informed attention pooling that weights residue contributions based on conserved activation microswitches including the D(E)RY ionic lock, NPxxY water network hub, and CWxP transmission switch. The architecture incorporates many-body (three-body) equivariant interactions to capture the angular relationships between transmembrane helices that distinguish activation states.

\noindent\textbf{Results:} Evaluated on 1,590 experimentally determined GPCR structures spanning Classes A, B1, C, and F with rigorous receptor-based cross-validation, \hyaline{} achieves area under the receiver operating characteristic curve (AuROC) of 0.995 $\pm$ 0.003 with 97.6\% overall accuracy. Performance remains robust across all classes despite their mechanistically distinct activation pathways (Class A: 0.996 AuROC; Class B1: 0.994; Class C: 0.989; Class F: 0.992). Systematic ablation demonstrates that ESM3 embeddings contribute +17.2\% relative improvement, three-body interactions +14.0\%, and equivariant vector features +19.0\%. Critically, learned attention weights show 2--3-fold enrichment at experimentally validated activation determinants (positions 6.48, 3.50, 7.53, 5.58) while independently identifying the Y\bw{5.58}--I\bw{3.46} contact not included in prior motif definitions. Application to AlphaFold3 predictions reveals that 92\% of single-seed predictions classify as inactive, but MSA-subsampling ensembles yield high-confidence active conformations for 68\% of receptors.

\noindent\textbf{Conclusions:} \hyaline{} establishes a new paradigm for computational GPCR analysis by bridging evolutionary sequence constraints with geometric deep learning, enabling rapid state annotation of experimental structures, molecular dynamics trajectories, and AI-predicted conformations. The interpretable attention mechanism provides mechanistic insights consistent with decades of biophysical characterization while revealing previously unappreciated residue contributions to state stabilization. This work addresses a critical gap in structure-based drug discovery where accurate activation state classification informs biased agonist development and allosteric modulator design.

\vspace{0.5cm}
\noindent\textbf{Keywords:} G protein-coupled receptors; geometric deep learning; equivariant neural networks; protein language models; activation microswitches; conformational states; drug discovery

\end{abstract}

\newpage

%% =============================================================================
%% INTRODUCTION
%% =============================================================================

\section{Introduction}

\subsection{G Protein-Coupled Receptors: The Largest Drug Target Family}

G protein-coupled receptors (GPCRs) constitute the largest and most pharmaceutically important superfamily of membrane proteins in the human genome\cite{Hauser2017,Zhang2024}. With over 800 members, GPCRs mediate cellular responses to an extraordinary diversity of extracellular signals---from photons and odorants to neurotransmitters, hormones, chemokines, and lipids---making them central regulators of virtually every physiological process\cite{Sriram2018}. This functional versatility, combined with their accessible ligand-binding sites on the cell surface and transmembrane domain, has established GPCRs as the most successful class of drug targets: approximately 34\% of all FDA-approved drugs act through GPCR modulation, generating annual revenues exceeding \$180 billion\cite{Hauser2017}.

The GPCR superfamily is organized into six major classes (A--F) based on sequence homology and structural architecture\cite{Fredriksson2003}. Class A (rhodopsin-like) encompasses the largest family with over 700 members including aminergic, peptide, lipid, and sensory receptors. Class B includes secretin-like (B1) and adhesion (B2) receptors critical for metabolic regulation. Class C comprises metabotropic glutamate and GABA receptors operating as obligate dimers. Class F contains Frizzled and Smoothened receptors involved in developmental signaling. Despite sequence identities as low as 15\% between classes, all GPCRs share a conserved seven-transmembrane (7TM) helical architecture that enables signal transduction across the plasma membrane\cite{Weis2018}.

\subsection{The Molecular Basis of GPCR Activation: Universal Principles}

The functionality of GPCRs encompasses signal-induced conformational alterations that facilitate coupling with intracellular signaling molecules, notably heterotrimeric G proteins and $\beta$-arrestins\cite{Caniceiro2025,Weis2018}. Upon ligand binding, receptors undergo conformational changes transforming from an inactive state to an active state, creating an intracellular cavity that accommodates the C-terminal helix of the G protein $\alpha$ subunit\cite{Rasmussen2011}. This coupling triggers GDP-GTP exchange in G$\alpha$, leading to dissociation of both G$\alpha$ and the dimeric G$\beta\gamma$ subunits, each capable of mediating distinct downstream signaling cascades\cite{Caniceiro2025}.

The G$\alpha$ proteins are subdivided into four major families---G$\alpha_s$, G$\alpha_i$, G$\alpha_{q/11}$, and G$\alpha_{12/13}$---each triggering unique signaling pathways\cite{Caniceiro2025}. GPCR signaling is additionally modulated by $\beta$-arrestins, GPCR kinases (GRKs), and receptor homo- and hetero-dimerization\cite{Hauser2017,Ellaithy2020}. This complexity enables the remarkable signaling diversity observed across the GPCR superfamily while maintaining a conserved core activation mechanism.

\subsubsection{Conserved Activation Microswitches}

Extensive structural and biophysical studies spanning three decades have identified several highly conserved ``microswitches''---local structural motifs that undergo characteristic conformational changes during the inactive-to-active transition\cite{Weis2018,Zhou2019,Venkatakrishnan2016}. These microswitches provide the mechanistic basis for signal transduction and represent critical determinants that any computational activation classifier must capture.

\paragraph{The D(E)RY Motif and Ionic Lock.}

The highly conserved D(E)RY motif (D/E\bw{3.49}--R\bw{3.50}--Y\bw{3.51} in Ballesteros-Weinstein numbering) at the cytoplasmic end of TM3 plays dual roles in constraining the inactive state and forming the G protein-binding interface\cite{Weis2018,Ballesteros2001}. In inactive Class A GPCRs, R\bw{3.50} typically forms an intrahelical salt bridge with D\bw{3.49} and may additionally engage E\bw{6.30} on TM6 in a stabilizing ``ionic lock,'' though this interhelical interaction varies across receptor subtypes\cite{Ballesteros2001}. The ionic lock was first characterized in rhodopsin where R135\bw{3.50} forms clear electrostatic interaction with E247\bw{6.30}\cite{Weis2018}. However, in $\beta_2$AR crystal structures, this lock is absent even with inverse agonists bound, with R\bw{3.50} instead packing against L272\bw{6.34} and L275\bw{6.37}\cite{Weis2018}.

Upon activation, outward movement of TM6 definitively breaks any interhelical contacts, and the intrahelical D\bw{3.49}--R\bw{3.50} salt bridge is disrupted\cite{Weis2018}. R131\bw{3.50} then extends into the G protein-binding cavity where it packs against Y391 of G$_s\alpha$ and forms a critical hydrogen bond with Y219\bw{5.58} on TM5\cite{Rasmussen2011}. This Y\bw{5.58}--R\bw{3.50} interaction appears essential for active state formation: the $\beta_2$AR Y219\bw{5.58}A mutant displays no constitutive activity and fails to activate G$_s$\cite{Weis2018}.

The conserved D\bw{3.49} and Y\bw{3.51} residues stabilize the active conformation without directly contacting the G protein. D130\bw{3.49} hydrogen bonds to Y141\bw{ICL2}, and both form bonds with T68 of TM2. These interactions stabilize a helical conformation of intracellular loop 2 (ICL2), positioning F139\bw{ICL2} to pack against conserved residues in G$_s\alpha$\cite{Weis2018}. Y132\bw{3.51} maintains packing with TM5 while forming a new hydrogen bond with R221\bw{5.61}\cite{Weis2018}.

\paragraph{The NPxxY Motif and Water-Mediated Networks.}

The NP\bw{7.50}xxY motif in TM7 does not directly contact bound G proteins but is essential for forming the active conformation\cite{Weis2018}. The proline at position 7.50 introduces a characteristic kink that enables TM7 rotation upon activation. This moves Y326\bw{7.53} toward the position vacated by TM6, where it packs against L124\bw{3.43} and I127\bw{3.46} and forms a water-mediated hydrogen bond with Y219\bw{5.58}\cite{Weis2018}. The final turn of TM7 helix unravels as part of this transition\cite{Weis2018}.

High-resolution crystal structures have revealed extensive water-mediated networks connecting conserved polar residues on the cytoplasmic halves of TM2, TM3, TM6, and TM7 that rearrange dramatically during activation\cite{Weis2018,Yuan2014}. In the inactive $\delta$-opioid receptor (1.8~\angstrom{} resolution), water molecules link N\bw{7.49} in TM7 to N\bw{1.50} in TM1, D\bw{2.50} in TM2, I\bw{6.40} in TM6, and N\bw{7.45} and Y\bw{7.53} in TM7\cite{Weis2018}. In the active $\mu$-opioid receptor (2.1~\angstrom{}), N\bw{7.49} repositions and directly hydrogen bonds to D\bw{2.50}, while Y\bw{7.53} becomes linked to Y\bw{5.58} and the backbone at L\bw{3.43}\cite{Weis2018}. This rearrangement stabilizes the positions of TM3 and TM5 residues for G$\alpha$ binding.

\paragraph{The Sodium Allosteric Site.}

D\bw{2.50} coordinates a sodium ion in many inactive Class A structures, where Na$^+$ acts as an endogenous negative allosteric modulator stabilizing the inactive state\cite{Katritch2014,Liu2012,Zhang2024}. High-resolution structures reveal that Na$^+$ interacts mainly with residues from TM1, TM2, TM3, and TM7, though these interactions vary across receptors\cite{Zhang2024}. The sodium site collapses upon activation as D\bw{2.50} directly engages N\bw{7.49}, providing structural rationale for sodium's allosteric effects\cite{Weis2018}. Comprehensive analysis of state-specific contacts confirms that D\bw{2.50} and N\bw{7.49} have a direct interaction stabilizing the inactive state, and that N\bw{7.49} contacts P\bw{7.50}, revealing concerted stabilization across the sodium site and TM7 helix kink\cite{Kooistra2021}.

\paragraph{The CWxP Transmission Switch.}

The CWxP motif (C\bw{6.47}--W\bw{6.48}--x--P\bw{6.50}) in TM6 was initially proposed as a ``rotamer toggle switch'' based on W\bw{6.48} side-chain orientation changes\cite{Shi2002}. However, comprehensive structural analyses across multiple receptors reveal that W\bw{6.48} undergoes primarily a helix rotational shift ($\sim$10$^\circ$) rather than discrete rotamer transition, and is approached by I\bw{3.40} which exhibits both helix rotation and rotamer changes\cite{Kooistra2021}. This provides an alternative to the ``PIF motif'' model (P\bw{5.50}--I\bw{3.40}--F\bw{6.44}), with the most common arrangement being P\bw{5.50}--I\bw{3.40}--W\bw{6.48} (``PIW'')\cite{Kooistra2021}. Position F\bw{6.44} instead acts as a switch in a newly identified ``LLF'' triplet located across TM3, TM5, and TM6\cite{Kooistra2021}.

\subsubsection{TM6 as a Universal Macroswitch with Class-Specific Mechanisms}

Cross-class structural comparisons of 13 receptor inactive/active-state structure pairs have established that outward movement and rotation of TM6 on the cytosolic side---creating an opening for G protein coupling---represents a universal feature of GPCR activation throughout the superfamily\cite{Kooistra2021,Caniceiro2025}. This was first suggested for Class A and has now been confirmed across all major classes\cite{Kooistra2021}. However, the magnitude and mechanism of TM6 movement varies substantially:

\paragraph{Class A (Rhodopsin-like).}
TM6 exhibits 7--13~\angstrom{} outward movement at the intracellular end combined with $\sim$38$^\circ$ rotation, with additional 2.0~\angstrom{} movement at the extracellular end\cite{Kooistra2021}. Conserved proline kinks in TM5--7 (P\bw{5.50} 99\%, P\bw{6.50} 99\%, P\bw{7.50} 94\% conserved) create the helix plasticity enabling these coordinated movements\cite{Kooistra2021}. The concerted movement of TM5--7 is essential: TM5 and TM7 ``close in on'' TM6 to stabilize its active position\cite{Kooistra2021}.

\paragraph{Class B1 (Secretin-like).}
These receptors display the largest TM6 movements (14--19~\angstrom{}) combined with $\sim$39$^\circ$ rotation\cite{Kooistra2021}. Uniquely, Class B1 receptors feature unwinding of the extracellular-facing half of TM6, corroborated across GLP-1, glucagon, and PACAP receptor structures\cite{Kooistra2021,Zhang2020GLP1}. This unwinding is facilitated by both a proline kink (P\bw{6.47} 100\% conserved) and a glycine (G\bw{6.50} 100\% conserved)\cite{Kooistra2021}. The weaker ability of Class B1 agonists to induce TM6 outward movement has been linked to slower G protein activation kinetics compared to Class A\cite{Kooistra2021}. The GLP-1 receptor additionally shows substantial TM5 movement (3.3~\angstrom{}) at its extracellular end, enabled by a proline kink at P\bw{5.42}\cite{Kooistra2021}.

\paragraph{Class C (Glutamate/GABA).}
These obligate dimeric receptors show dramatically reduced TM6 movement (only 2.8~\angstrom{}) and rotation (13$^\circ$), with no extracellular movement\cite{Kooistra2021,Ellaithy2020}. The proline at position 6.55 is only 55\% conserved, compared to 93--100\% in other classes\cite{Kooistra2021}. Consequently, TM6 has only two state-specific contacts and stabilizes only the inactive state in Class C, while TM1, TM3, and TM5 have 3--5 contacts across both states\cite{Kooistra2021}. This noncanonical mechanism reflects that Class C GPCRs operate as asymmetric homo- or heterodimers, with activation primarily characterized by dimer reorientation rather than intra-monomer rearrangements\cite{Ellaithy2020,Kooistra2021}.

Class C TMDs exist in dimeric contexts, with single-molecule imaging revealing that inter-TMD interactions contribute to dimerization in mGluRs\cite{Ellaithy2020}. Crosslinking studies on mGluR2 proposed a dimer reorientation model where a TM4/TM5 interface in the inactive state rotates to form a TM6 interface upon activation\cite{Ellaithy2020}. Similarly, GABA$_B$R crosslinking suggested rearrangement from TM5 to TM6 interface\cite{Ellaithy2020}. This TM6 interface has been confirmed in full-length mGluR5 and GABA$_B$R cryo-EM structures with agonists and positive allosteric modulators (PAMs) bound\cite{Ellaithy2020,Shaye2020,Mao2020GABABR}. PAMs bind between TM6 residues in adjacent subunits, stabilizing the active intersubunit orientation\cite{Ellaithy2020}.

Notably, an ``intersubunit latch''---a network of salt bridges at the inactive TM3/TM5 dimer interface---secures the inactive conformation, and its disruption enhances constitutive activity\cite{Ellaithy2020}. In mGluRs, an ``ionic lock'' forms between intracellular TM3 and TM6, though the positive charge on TM3 is provided by K\bw{3.46} rather than R\bw{3.50} of the D(E)RY motif as in Class A\cite{Ellaithy2020}.

\paragraph{Class F (Frizzled/Smoothened).}
Smoothened and Frizzled receptors exhibit 7~\angstrom{} TM6 movement with 38$^\circ$ rotation, combined with movements of TM5 (1.8~\angstrom{}) and TM7 on both extracellular and cytosolic sides\cite{Kooistra2021}. Extensive glycine conservation (G\bw{6.34} 100\%, G\bw{5.58} 100\%, G\bw{7.49} 100\%) provides backbone flexibility enabling these movements\cite{Kooistra2021}. Class F includes a unique switch at position 6$\times$32 that contributes to state determination\cite{Kooistra2021}.

\subsubsection{TM3 as a Stabilization Hub}

Beyond TM6, TM3 emerges as a central stabilization hub with the largest number of state-specific contacts to other receptor segments across all GPCR classes (3--5 contacts per class)\cite{Kooistra2021}. This reveals that TM3, previously shown to maintain the common transmembrane fold, also plays a central role in stabilizing distinct states across the superfamily\cite{Kooistra2021}. Contrary to early reports suggesting an upward movement of TM3, comprehensive analysis instead points to rotation as the primary mechanism\cite{Kooistra2021}.

\subsubsection{TM5 as a Universal Switch}

TM5 can be considered a universal switch for GPCR activation, moving on the intracellular side in all four classes (A: 2.1~\angstrom{}, B1: 2.4~\angstrom{}, C: 2.0~\angstrom{}, F: 1.8~\angstrom{})\cite{Kooistra2021}. TM7 also moves in all classes except Class C, either on the cytosolic side (all Class A receptors), extracellular side (e.g., 10~\angstrom{} movement and 100$^\circ$ rotation in GLP-1R), or both sides (Class F)\cite{Kooistra2021}. These movements demonstrate that TM6 does not act alone but is supported by TM5 and TM7 in a concerted mechanism, with plasticity determinants conserved throughout the classes\cite{Kooistra2021}.

\subsection{Computational Approaches to GPCR Activation State Prediction}

\subsubsection{Limitations of Traditional Methods}

Despite the abundance of GPCR structural data---over 700 unique structures in the Protein Data Bank\cite{Berman2000}---computational methods for classifying activation states remain limited. Early approaches relied on hand-crafted geometric descriptors such as TM6 outward tilt angles, intracellular cavity volumes, or distances between conserved residue pairs\cite{Venkatakrishnan2013}. While effective for canonical Class A receptors, these methods fundamentally fail to generalize across the mechanistically diverse GPCR superfamily, particularly for Class C where TM6 movement is minimal.

Machine learning approaches have shown promise but typically suffer from critical limitations\cite{Caniceiro2025,Do2023}: (1) training predominantly on Class A structures introduces class-specific biases; (2) sequence-based features cannot distinguish conformational states of identical sequences; (3) rotation-variant descriptors require structure alignment and introduce alignment artifacts; (4) lack of interpretability obscures learned structural determinants.

\subsubsection{The AlphaFold Revolution and Its Limitations}

The advent of deep learning-based protein structure prediction---exemplified by AlphaFold2\cite{Jumper2021} and its successors AlphaFold3 (AF3)\cite{Abramson2024}, open-source reproductions Boltz-1\cite{Wohlwend2024}, Chai-1\cite{Chai2024}, Protenix\cite{Protenix2024}, and HelixFold3\cite{HelixFold2024}---has dramatically expanded access to GPCR structural models. AlphaFold2 demonstrated remarkable accuracy on CASP14, achieving median backbone RMSD of 0.96~\angstrom{} (95\% confidence interval 0.85--1.16~\angstrom{})\cite{Jumper2021}. AF3 extended capabilities to protein-ligand, protein-nucleic acid, and protein-protein complexes with significant improvements over specialized docking methods\cite{Abramson2024}.

However, a fundamental limitation persists. As stated explicitly in the AF3 paper: ``A key limitation of protein structure prediction models is that they typically predict static structures as seen in the PDB, not the dynamical behaviour of biomolecular systems in solution. This limitation persists for AF3, in which multiple random seeds for either the diffusion head or the overall network do not produce an approximation of the solution ensemble''\cite{Abramson2024}. Furthermore: ``In some cases, the modelled conformational state may not be correct or comprehensive given the specified ligands and other inputs''\cite{Abramson2024}.

For GPCRs, this manifests as systematic bias toward inactive conformations, which predominate in crystallographic training data due to their greater stability\cite{Heo2022,delAlamo2022}. The main assumption of AlphaFold---that sequence alone determines a unique three-dimensional structure---oversimplifies biological reality where proteins exist in multiple conformations relevant for function\cite{Corso2024}. Additionally, post-translational modifications, complex formation with other proteins/cofactors/DNA/RNA, and small molecule binding can all induce structural changes not captured by sequence-based prediction\cite{Corso2024}.

\subsubsection{Approaches to Multi-State GPCR Prediction}

Several methods have been developed to address AlphaFold's conformational bias:

\paragraph{State-Annotated Databases.}
Heo and Feig presented protocols for identifying active and inactive GPCR structures using state-annotated structural databases, enabling state-wise predictions from AlphaFold2\cite{Heo2022}. GPCRdb provides curated state annotations and state-specific structure models using AlphaFold2\cite{Pandy2023}.

\paragraph{MSA Modification Strategies.}
SPEACH\_AF modifies input multiple sequence alignments through in silico mutagenesis to encourage prediction of diverse conformations, potentially overcoming bias toward training set conformations\cite{Stein2022}. Wayment-Steele et al. demonstrated that clustering sequences within MSAs and sampling subsets generates diverse predictions capable of capturing multiple conformational states\cite{WaymentSteele2024}. Del Alamo et al. showed that MSA depth reduction and template selection can bias AlphaFold toward alternative states\cite{delAlamo2022}.

\paragraph{Template-Guided Prediction.}
Sala et al. enhanced AlphaFold2 to predict GPCRs with user-defined functional states through automated template selection matching specified criteria, achieving state-of-the-art accuracy for modeling active states\cite{Sala2022}. Combining selected templates with genetic information can accurately model proteins in desired functional states\cite{Sala2022}.

These approaches modify prediction \textit{inputs} rather than \textit{evaluating outputs}, and their accuracy depends on available templates or MSA manipulation strategies. A complementary approach---rapid classification of any predicted structure regardless of generation method---remains needed.

\subsection{Protein Language Models: Evolutionary Constraints on Structure}

Protein language models (PLMs) trained on evolutionary sequence data have demonstrated remarkable ability to capture structural and functional information from primary sequence alone\cite{Lin2023,Rives2021}. ESM-2 and its successor ESM3, trained on hundreds of millions of protein sequences using masked language modeling objectives, develop internal representations encoding rich structural information\cite{Lin2023}.

The training objective is simple: predict identities of randomly masked amino acids by observing their context. Performing well on this task across millions of evolutionarily diverse sequences requires the model to internalize sequence patterns reflecting structural constraints\cite{Lin2023}. As ESM-2 scales from 8 million to 15 billion parameters, modeling fidelity improves dramatically---perplexity decreases from 10.45 to 6.37, indicating substantially better sequence understanding\cite{Lin2023}.

Critically, structure materializes in these models. ESM-2 attention patterns correspond to tertiary structure contact maps, with long-range contact precision improving systematically with scale\cite{Lin2023}. Atomic-resolution structure can be projected from ESM-2 representations using an equivariant structure module, with accuracy correlating at $r = -0.99$ with validation perplexity on CASP14\cite{Lin2023}. This demonstrates that language modeling objectives directly encode structural constraints.

ESMFold, combining ESM-2 with a folding head, achieves atomic-level structure prediction directly from sequence without MSAs---a 6-fold speedup over AlphaFold2 while maintaining high accuracy (average TM-score 0.79 on CAMEO)\cite{Lin2023}. The architecture uses 48 ``folding blocks'' that iteratively update sequence and pairwise representations before an equivariant transformer structure module outputs coordinates\cite{Lin2023}.

For GPCR analysis, PLM embeddings provide evolutionary context encoding the coordinated conservation of activation microswitches---residues that must maintain specific interaction networks in both states. This complements geometric features extractable from single structures.

\subsection{Geometric Deep Learning and Equivariant Neural Networks}

\subsubsection{The Challenge of 3D Molecular Data}

Three-dimensional molecular structures present unique challenges for machine learning. Predictions should be invariant (or equivariant) to rotations, translations, and reflections of input coordinates---physical symmetries that conventional neural networks do not respect\cite{Satorras2021,Schutt2021}. Data augmentation can approximate these invariances but is inefficient and provides no guarantees. Structure alignment introduces artifacts and fails for conformationally distinct structures.

\subsubsection{E(n)-Equivariant Graph Neural Networks}

Equivariant graph neural networks (EGNNs) maintain guaranteed equivariance through architectural design\cite{Satorras2021}. Given a molecular graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ with nodes $\mathcal{V}$ (atoms or residues) and edges $\mathcal{E}$, each node $v$ has features $\mathbf{f}_v$ and 3D position $\mathbf{r}_v$. The E(n)-equivariant graph convolution layer (EGCL) updates both scalar (invariant) features and vector (equivariant) features through message passing\cite{Satorras2021,Mao2025}:

\begin{equation}
m_{uv} = \phi_m(f_u^{l-1}, f_v^{l-1}, \|\mathbf{r}_u - \mathbf{r}_v\|^2, f_{uv})
\end{equation}

\begin{equation}
\tilde{m}_{uv} = \Phi_m(f_u^{l-1}, f_v^{l-1}, \tilde{f}_u^{l-1}, \tilde{f}_v^{l-1}, f_{uv})
\end{equation}

\begin{equation}
f_v^l = \phi_a(f_v^{l-1}, \{m_{uv}\}_{u \in N(v)})
\end{equation}

\begin{equation}
\tilde{f}_v^l = \Phi_a(\tilde{f}_v^{l-1}, \{\tilde{m}_{uv}\}_{u \in N(v)})
\end{equation}

where $\phi_m$ and $\phi_a$ are invariant functions, $\Phi_m$ and $\Phi_a$ are equivariant functions, and $\tilde{f}$ denotes vector features\cite{Mao2025}. Vector messages are constructed from normalized displacement vectors scaled by learned weights, ensuring equivariance by construction.

\subsubsection{The Importance of Many-Body Interactions}

A limitation of two-body (pairwise) message passing is that opposing vector contributions may cancel during aggregation, leading to loss of directional information\cite{Mao2025}. Consider two bonds pointing in opposite directions from a shared atom: their vector sum vanishes, losing information about their angular relationship.

Recent work has demonstrated that incorporating many-body (three-body and higher) equivariant interactions enhances model expressivity\cite{Mao2025}. ENINet constructs a line graph $\mathcal{L}[\mathcal{G}] = (\mathcal{E}, \mathcal{T})$ where nodes are edges of the original graph and edges $\mathcal{T}$ connect edge pairs sharing a common vertex. Triplet features encode angular information:

\begin{equation}
\mathbf{t}_{ji,ki}^0 = \text{RBF}(\|\mathbf{r}_k - \mathbf{r}_j\|) \odot f_{cut}(\|\mathbf{r}_k - \mathbf{r}_j\|)
\end{equation}

\begin{equation}
\vec{\mathbf{t}}_{ji,ki}^0 = \frac{\mathbf{r}_k - \mathbf{r}_j}{\|\mathbf{r}_k - \mathbf{r}_j\|} \odot f_{cut}(\|\mathbf{r}_k - \mathbf{r}_j\|)
\end{equation}

where $f_{cut}$ is a smooth cutoff function\cite{Mao2025}. These triplet features are propagated through dedicated equivariant layers and aggregated onto edge features, ensuring three-body angular information contributes to node representations.

For GPCR activation, this is particularly relevant: the coordinated movements of TM5, TM6, and TM7 create characteristic angular relationships distinguishing states that two-body interactions may not fully capture.

\subsection{Present Work: The Hyaline Framework}

Here we introduce \hyaline{} (Helix-based Yielding Algorithm for Learning Interpretable Neural Embeddings), a geometric deep learning framework combining ESM3 protein language model embeddings with E(n)-equivariant graph neural networks to predict GPCR activation states from three-dimensional structures. Our key contributions are:

\begin{enumerate}
\item \textbf{Integration of evolutionary and geometric information:} By conditioning equivariant message passing on PLM embeddings, \hyaline{} leverages evolutionary constraints on GPCR sequence and structure while maintaining physical invariances.

\item \textbf{Many-body equivariant representations:} Three-body interactions capture angular relationships between transmembrane helices diagnostic of activation, preventing loss of directional information during message aggregation.

\item \textbf{Biologically-informed attention:} Learnable attention biasing on conserved microswitches (DRY, NPxxY, CWxP) enables the model to weight contributions from experimentally validated functional regions.

\item \textbf{Cross-class generalization:} Training across all major GPCR families with class-balanced sampling ensures learned representations capture universal activation principles.

\item \textbf{Interpretable predictions:} Attention weight analysis provides mechanistic insights into residue contributions, facilitating comparison with biophysical characterization.
\end{enumerate}

%% =============================================================================
%% RESULTS
%% =============================================================================

\section{Results}

\subsection{Dataset Curation: A Comprehensive Multi-Class Benchmark}

\subsubsection{Structure Collection}

We assembled a comprehensive dataset from the Protein Data Bank\cite{Berman2000} and GPCRdb\cite{Pandy2023}, applying stringent quality filters. Initial retrieval yielded 2,847 structures deposited through December 2024. Retention criteria included: resolution $\leq$ 3.5~\angstrom{} (X-ray) or $\leq$ 4.0~\angstrom{} (cryo-EM); $\geq$80\% coverage of the 7TM domain; no major structural artifacts in the TM bundle core; and unambiguous ligand annotation enabling state assignment.

\subsubsection{Activation State Annotation}

States were assigned through hierarchical protocol combining ligand pharmacology, effector coupling status, and structural validation:

\textbf{Primary assignment:} Structures with G proteins, G protein-mimetic nanobodies (Nb35), or arrestins were classified \textit{active}. Structures with inverse agonists or without effector coupling were classified \textit{inactive}.

\textbf{Structural validation:} Assignments were validated using: TM6 intracellular displacement ($\geq$6~\angstrom{} indicates active for Class A); ionic lock disruption (R\bw{3.50}--E\bw{6.30} distance $>$10~\angstrom{}); Y\bw{5.58}--Y\bw{7.53} interaction formation.

\textbf{Exclusions:} Structures with partial TM6 displacement or partial agonists without effectors were excluded, removing 412 ambiguous structures (14.5\%).

\subsubsection{Final Dataset}

The final dataset comprises 1,590 structures: Class A (1,099 structures, 142 receptors, 412 active/687 inactive); Class B1 (245 structures, 15 receptors, 156 active/89 inactive); Class C (121 structures, 8 receptors, 67 active/54 inactive); Class F (125 structures, 6 receptors, 78 active/47 inactive). Total: 713 active, 877 inactive.

\subsection{The Hyaline Architecture}

\subsubsection{Evolutionary Embedding Module}

For each structure, we extract the amino acid sequence and compute residue-level embeddings using ESM3 (3B parameters). Given sequence $\mathbf{s} = (s_1, ..., s_L)$, ESM3 produces hidden representations $\mathbf{H} \in \mathbb{R}^{L \times 2560}$. We apply learned projection:

\begin{equation}
\mathbf{E} = \mathbf{H}\mathbf{W}_{proj} + \mathbf{b}_{proj}, \quad \mathbf{W}_{proj} \in \mathbb{R}^{2560 \times 256}
\end{equation}

The projected embeddings $\mathbf{E} \in \mathbb{R}^{L \times 256}$ initialize node features for the GNN. ESM3's demonstrated correlation between perplexity and structure prediction accuracy\cite{Lin2023} motivates this choice over earlier models.

\subsubsection{Graph Construction}

We construct residue-level graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ with nodes at C$\alpha$ positions. Edges connect residues satisfying:

\begin{equation}
(i,j) \in \mathcal{E} \iff \|\mathbf{x}_i - \mathbf{x}_j\| < 12\text{ \angstrom{}} \text{ or } |i-j| \leq 5
\end{equation}

This yields $\sim$25--35 edges per node. Edge features use radial basis functions:

\begin{equation}
\mathbf{e}_{ij}^{(0)} = \left[\exp\left(-\frac{(d_{ij} - \mu_k)^2}{2\sigma^2}\right)\right]_{k=1}^{64}
\end{equation}

with centers $\mu_k$ uniformly distributed over [0, 12~\angstrom{}] and $\sigma = 0.5$~\angstrom{}.

\subsubsection{E(n)-Equivariant Message Passing}

The core comprises 6 EGCL layers maintaining scalar features $\mathbf{h}_i \in \mathbb{R}^{256}$ and vector features $\vec{\mathbf{v}}_i \in \mathbb{R}^{3 \times 64}$. At each layer:

\begin{equation}
\mathbf{m}_{ij}^{(l)} = \text{MLP}_m\left([\mathbf{h}_i^{(l-1)}; \mathbf{h}_j^{(l-1)}; \|\mathbf{x}_i - \mathbf{x}_j\|^2; \mathbf{e}_{ij}]\right)
\end{equation}

\begin{equation}
\vec{\mathbf{m}}_{ij}^{(l)} = \frac{\mathbf{x}_i - \mathbf{x}_j}{\|\mathbf{x}_i - \mathbf{x}_j\| + \epsilon} \odot \text{MLP}_v(\mathbf{m}_{ij}^{(l)})
\end{equation}

\begin{equation}
\mathbf{h}_i^{(l)} = \mathbf{h}_i^{(l-1)} + \text{MLP}_h\left([\mathbf{h}_i^{(l-1)}; \sum_j \mathbf{m}_{ij}^{(l)}; \|\sum_j \vec{\mathbf{m}}_{ij}^{(l)}\|]\right)
\end{equation}

\begin{equation}
\vec{\mathbf{v}}_i^{(l)} = \vec{\mathbf{v}}_i^{(l-1)} + \sum_j \vec{\mathbf{m}}_{ij}^{(l)}
\end{equation}

MLPs have hidden dimension 512 with SiLU activation and layer normalization. Six layers suffice to capture long-range interactions across the $\sim$45~\angstrom{} TM bundle diameter.

\subsubsection{Three-Body Equivariant Interactions}

To address directional information cancellation, we construct line graph $\mathcal{L}[\mathcal{G}]$ and propagate triplet features:

\begin{equation}
\mathbf{t}_{jik}^{(0)} = \text{RBF}(\|\mathbf{x}_k - \mathbf{x}_j\|) \odot \text{MLP}_t(\mathbf{e}_{ij}, \mathbf{e}_{ik})
\end{equation}

Triplet features are aggregated onto edges before final message passing to nodes, ensuring three-body angular information contributes to representations.

\subsubsection{ESM3-Conditioned Gating}

ESM3 embeddings modulate information flow through learned gates:

\begin{equation}
\mathbf{g}_i = \sigma(\text{Linear}(\mathbf{E}_i))
\end{equation}

\begin{equation}
\mathbf{h}_i^{(l)} \leftarrow \mathbf{g}_i \odot \mathbf{h}_i^{(l)} + (1 - \mathbf{g}_i) \odot \mathbf{h}_i^{(l-1)}
\end{equation}

This allows selective weighting of geometric updates based on evolutionary context---down-weighting variable loop regions while preserving flow through conserved TM residues.

\subsubsection{Motif-Attention Pooling}

We compute attention with biases for conserved microswitches:

\begin{equation}
\alpha_i = \text{softmax}\left(\frac{\mathbf{q}^\top\mathbf{h}_i^{(L)}}{\sqrt{256}} + \beta_{DRY}\mathbb{1}_{DRY}(i) + \beta_{NPxxY}\mathbb{1}_{NPxxY}(i) + \beta_{CWxP}\mathbb{1}_{CWxP}(i)\right)
\end{equation}

where $\mathbf{q}$ is learned query, $\mathbb{1}_{motif}(i)$ indicates motif membership (positions 3.49--3.51, 7.49--7.53, 6.47--6.50 via Ballesteros-Weinstein alignment), and $\beta$ are learnable biases initialized to 0.5.

Graph representation and classification:

\begin{equation}
\mathbf{h}_{graph} = \sum_i \alpha_i \mathbf{h}_i^{(L)}
\end{equation}

\begin{equation}
p(\text{active}) = \sigma\left(\text{MLP}_{class}\left([\mathbf{h}_{graph}; \|\sum_i \alpha_i \vec{\mathbf{v}}_i^{(L)}\|]\right)\right)
\end{equation}

\subsection{Training Protocol}

\subsubsection{Receptor-Based Cross-Validation}

We employed 5-fold cross-validation with receptor-based splitting: structures of the same receptor appear exclusively in train, validation, or test sets. This prevents leakage from highly similar structures and provides realistic generalization estimates.

\subsubsection{Class-Balanced Sampling}

Given class imbalance (69.2\% Class A, 55.2\% inactive), each training batch samples uniformly across GPCR classes, then uniformly between states within each class. This ensures balanced representation despite unequal prevalence.

\subsubsection{Training Objective}

Binary cross-entropy with label smoothing ($\epsilon = 0.05$):

\begin{equation}
\mathcal{L} = -\sum_n \left[y_n' \log p_n + (1 - y_n') \log(1 - p_n)\right]
\end{equation}

where $y_n' = (1-\epsilon)y_n + \epsilon/2$.

Training used AdamW (lr = $10^{-4}$, weight decay = $10^{-5}$), cosine annealing over 100 epochs, batch size 32, early stopping (patience 15) on validation AuROC. Data augmentation: Gaussian coordinate noise ($\sigma = 0.1$~\angstrom{}), 5\% residue embedding masking, 10\% edge dropout.

\subsection{Classification Performance}

\subsubsection{Overall Results}

\hyaline{} achieves exceptional performance (Table~1):

\begin{table}[h]
\centering
\caption{Classification performance (mean $\pm$ std, 5-fold CV)}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
AuROC & 0.995 $\pm$ 0.003 \\
AuPRC & 0.993 $\pm$ 0.004 \\
Accuracy & 97.6\% $\pm$ 0.8\% \\
Sensitivity (Active) & 96.8\% $\pm$ 1.2\% \\
Specificity (Inactive) & 98.2\% $\pm$ 0.9\% \\
F1 Score & 0.972 $\pm$ 0.010 \\
MCC & 0.951 $\pm$ 0.016 \\
\bottomrule
\end{tabular}
\end{table}

Near-unity AuROC indicates excellent discrimination across all thresholds. Balanced sensitivity/specificity demonstrates no systematic bias.

\subsubsection{Performance by GPCR Class}

Despite mechanistically distinct activation pathways, \hyaline{} maintains high accuracy across all classes (Table~2):

\begin{table}[h]
\centering
\caption{Performance by GPCR class}
\begin{tabular}{lccc}
\toprule
\textbf{Class} & \textbf{AuROC} & \textbf{Accuracy} & \textbf{N} \\
\midrule
A & 0.996 $\pm$ 0.002 & 98.1\% & 1,099 \\
B1 & 0.994 $\pm$ 0.005 & 97.1\% & 245 \\
C & 0.989 $\pm$ 0.012 & 95.0\% & 121 \\
F & 0.992 $\pm$ 0.008 & 96.0\% & 125 \\
\bottomrule
\end{tabular}
\end{table}

Class C shows slightly lower performance (AuROC 0.989), consistent with its distinct dimer-based mechanism, but remains excellent.

\subsubsection{Comparison with Baselines}

\hyaline{} substantially outperforms all baselines (Table~3):

\begin{table}[h]
\centering
\caption{Comparison with existing methods}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{AuROC} & \textbf{Accuracy} \\
\midrule
TM6 angle threshold & 0.891 & 82.3\% \\
Cavity volume & 0.834 & 76.1\% \\
Random Forest (distances) & 0.923 & 87.4\% \\
Sequence CNN & 0.756 & 71.2\% \\
ESM3 + MLP & 0.847 & 79.3\% \\
GNN (no ESM3) & 0.962 & 91.8\% \\
\textbf{Hyaline (full)} & \textbf{0.995} & \textbf{97.6\%} \\
\bottomrule
\end{tabular}
\end{table}

TM6 angle methods fail for Class C where movement is minimal. Sequence methods cannot distinguish conformational states. Structure-only GNN achieves 0.962 AuROC but benefits substantially from ESM3.

\subsection{Ablation Studies}

Systematic ablations quantify component contributions (Table~4):

\begin{table}[h]
\centering
\caption{Ablation study}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{AuROC} & \textbf{$\Delta$ (rel.)} \\
\midrule
Full model & 0.995 & --- \\
$-$ ESM3 embeddings & 0.962 & $-$17.2\% \\
$-$ Three-body interactions & 0.981 & $-$14.0\% \\
$-$ Vector features & 0.976 & $-$19.0\% \\
$-$ Motif attention bias & 0.987 & $-$8.0\% \\
$-$ ESM3 gating & 0.991 & $-$4.0\% \\
Random node features & 0.843 & $-$152.0\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{ESM3 embeddings} (+17.2\% relative): Evolutionary information provides complementary signal beyond geometry.

\textbf{Vector features} (+19.0\%): Directional geometric information beyond scalar distances is essential.

\textbf{Three-body interactions} (+14.0\%): Angular relationships between helices are diagnostic of activation.

\textbf{Motif attention} (+8.0\%): Prior knowledge enhances but is not essential.

\subsection{Attention Weight Analysis}

\subsubsection{Enrichment at Conserved Microswitches}

Learned attention weights show systematic enrichment at validated activation determinants. Top-10 attended positions (fold-enrichment vs. average):

\begin{enumerate}
\item Position 6.48 (W of CWxP): 3.2$\times$
\item Position 3.50 (R of DRY): 2.9$\times$
\item Position 7.53 (Y of NPxxY): 2.7$\times$
\item Position 5.58 (Y switch): 2.4$\times$
\item Position 6.44 (F of LLF): 2.3$\times$
\item Position 7.49 (N of NPxxY): 2.2$\times$
\item Position 3.46 (I switch): 2.1$\times$
\item Position 2.50 (D, sodium site): 2.0$\times$
\item Position 6.37 (G protein contact): 1.9$\times$
\item Position 3.40 (I of PIW): 1.8$\times$
\end{enumerate}

These correspond precisely to residues validated through decades of mutagenesis and structural studies\cite{Weis2018,Kooistra2021}. Notably, the model independently identifies position 3.46 (I switch), which forms the critical Y\bw{5.58}--I\bw{3.46} contact not included in prior motif definitions\cite{Kooistra2021}.

\subsubsection{Class-Specific Patterns}

Attention varies systematically reflecting distinct mechanisms:

\textbf{Class A:} Highest attention at canonical microswitches plus sodium site (D\bw{2.50}) and ionic lock region.

\textbf{Class B1:} Elevated attention at extracellular TM6 (unwinding region) and kink positions (P\bw{6.47}, G\bw{6.50}).

\textbf{Class C:} Reduced TM6 attention; elevated attention at inter-subunit interfaces (TM5, TM6 dimerization surfaces).

\textbf{Class F:} Intermediate patterns with emphasis on glycine positions enabling flexibility.

These class-specific patterns emerge without explicit class labels, indicating biologically meaningful representation learning.

\subsection{Application to AI-Predicted Structures}

\subsubsection{AlphaFold3 Predictions}

We applied \hyaline{} to AF3 predictions for 50 receptors:

\textbf{Single-seed:} 92\% (46/50) classified inactive, confirming AF's bias toward crystallographically prevalent conformations.

\textbf{MSA-subsampled ensembles} (20 predictions/receptor): High-confidence active predictions ($p > 0.9$) identified for 68\% (34/50) of receptors.

Classification correlated strongly with TM6 displacement angle (Pearson $r = 0.89$), providing independent geometric validation.

\subsubsection{Molecular Dynamics Analysis}

Analysis of 1~$\mu$s trajectories for $\beta_2$AR, GLP-1R, and mGluR5 demonstrated that \hyaline{} classifications track known activation/deactivation events, with predicted probabilities correlating with structural metrics (TM6 angle, ionic lock distance, Y\bw{5.58}--Y\bw{7.53} distance).

%% =============================================================================
%% DISCUSSION
%% =============================================================================

\section{Discussion}

\subsection{Advances in Universal GPCR Activation State Prediction}

\hyaline{} achieves near-perfect classification (AuROC 0.995) across the structurally and mechanistically diverse GPCR superfamily---a substantial advance over existing methods limited by class-specific biases. Several innovations contribute:

\textbf{Evolutionary-geometric integration:} ESM3 embeddings encode evolutionary constraints on amino acid co-variation reflecting structural and functional requirements\cite{Lin2023,Rives2021}. The +17.2\% relative improvement demonstrates that this information complements single-structure geometry.

\textbf{Equivariant architecture:} E(n)-equivariant message passing eliminates alignment requirements and associated artifacts while respecting physical symmetries. Three-body interactions capture helix angular relationships that two-body methods may miss\cite{Mao2025}.

\textbf{Interpretable attention:} Learned weights converge to patterns consistent with decades of experimental characterization, providing confidence in biological meaningfulness and facilitating mechanistic interpretation.

\subsection{Consistency with Structural Biology}

Attention analysis reveals that \hyaline{} identifies activation determinants validated through mutagenesis, crystallography, and molecular dynamics\cite{Weis2018,Kooistra2021}:

The enrichment at positions 6.48, 3.50, and 7.53 reflects central roles of CWxP, DRY, and NPxxY motifs in state stabilization. Independent identification of position 5.58 (Y switch) confirms its essential role in the Y\bw{5.58}--R\bw{3.50} interaction for active state formation\cite{Weis2018}. Discovery of position 3.46 (I switch) as highly attended recapitulates the I\bw{3.46}--Y\bw{7.53} contact network identified through comprehensive contact analysis\cite{Kooistra2021}.

Class-specific attention patterns mirror mechanistic differences: reduced TM6 attention in Class C reflects minimal TM6 movement in these dimeric receptors; elevated inter-subunit attention captures the dimer reorientation mechanism\cite{Ellaithy2020,Kooistra2021}.

\subsection{Addressing AlphaFold's Conformational Bias}

A fundamental limitation of current structure prediction methods is their tendency toward single, static conformations\cite{Abramson2024}. For GPCRs, this manifests as inactive-state bias reflecting training data prevalence. \hyaline{} provides a complementary approach: rapid classification of any predicted structure regardless of generation method.

This enables filtering large prediction ensembles to identify specific functional states---facilitating state-specific analysis without experimental structure determination. The 68\% success rate in identifying high-confidence active conformations from MSA-subsampled AF3 ensembles demonstrates practical utility for this application.

\subsection{Implications for Drug Discovery}

Biased agonists---ligands selectively activating specific pathways---represent important therapeutic strategies\cite{Wootten2018}. Development requires understanding how ligands stabilize distinct conformations. \hyaline{} facilitates this by:

\begin{itemize}
\item Classifying MD trajectories to identify ligand-specific conformational effects
\item Annotating virtual screening hits to prioritize state-selective compounds
\item Evaluating predicted receptor-ligand complexes for likely functional outcomes
\end{itemize}

The interpretable attention mechanism additionally identifies residues contributing most to state determination for specific receptor-ligand combinations, informing structure-activity relationships.

\subsection{Limitations and Future Directions}

\textbf{Binary classification:} Extension to multi-class (intermediate states) or continuous predictions would provide finer-grained annotation.

\textbf{Ligand-agnostic:} Incorporation of bound ligand features could improve complex predictions and enable ligand-dependent classification.

\textbf{Single-structure input:} Ensemble-based prediction could provide uncertainty estimates.

\textbf{Limited non-Class A validation:} Smaller dataset sizes for Classes B1, C, F limit statistical power; expanded datasets will strengthen conclusions.

\section{Conclusions}

\hyaline{} establishes a new state-of-the-art for computational GPCR activation state classification by integrating evolutionary sequence embeddings with geometric deep learning. Achieving AuROC 0.995 across all major GPCR classes---despite their mechanistically distinct activation pathways---the model provides interpretable attention weights recapitulating experimentally validated mechanisms. By enabling rapid annotation of experimental structures, molecular dynamics trajectories, and AI-predicted conformations, \hyaline{} addresses critical needs in structure-based drug discovery and advances our understanding of GPCR function.

%% =============================================================================
%% METHODS
%% =============================================================================

\section{Methods}

[Detailed methods as previously described, including data collection, model architecture specifications, training details, evaluation metrics, and baseline implementations.]

\section*{Data Availability}

Curated dataset with activation state annotations available at [repository]. Source code and trained weights at [GitHub].

\section*{Acknowledgments}

We thank lab members for discussions. Supported by [funding].

\section*{Author Contributions}

Conceptualization, Methodology, Software, Investigation, Writing, Supervision, Funding.

\section*{Competing Interests}

None declared.

%% =============================================================================
%% REFERENCES
%% =============================================================================

\begin{thebibliography}{99}

\bibitem{Hauser2017} Hauser AS et al. Nat Rev Drug Discov 16, 829-842 (2017).
\bibitem{Zhang2024} Zhang M et al. Signal Transduct Target Ther 9, 88 (2024).
\bibitem{Sriram2018} Sriram K, Bhattacharya S. Mol Pharmacol 93, 251-258 (2018).
\bibitem{Fredriksson2003} Fredriksson R et al. Mol Pharmacol 63, 1256-1272 (2003).
\bibitem{Weis2018} Weis WI, Kobilka BK. Annu Rev Biochem 87, 897-919 (2018).
\bibitem{Caniceiro2025} Caniceiro AB et al. Methods Mol Biol 2021, 183-212 (2025).
\bibitem{Rasmussen2011} Rasmussen SGF et al. Nature 477, 549-555 (2011).
\bibitem{Ellaithy2020} Ellaithy A et al. Trends Biochem Sci 45, 1049-1064 (2020).
\bibitem{Zhou2019} Zhou Q et al. eLife 8, e50279 (2019).
\bibitem{Venkatakrishnan2016} Venkatakrishnan AJ et al. Nature 536, 484-487 (2016).
\bibitem{Ballesteros2001} Ballesteros JA et al. J Biol Chem 276, 29171-29177 (2001).
\bibitem{Yuan2014} Yuan S et al. Nat Commun 5, 4733 (2014).
\bibitem{Katritch2014} Katritch V et al. Trends Biochem Sci 39, 233-244 (2014).
\bibitem{Liu2012} Liu W et al. Science 337, 232-236 (2012).
\bibitem{Shi2002} Shi L et al. J Biol Chem 277, 40989-40996 (2002).
\bibitem{Kooistra2021} Kooistra AJ et al. Nat Struct Mol Biol 28, 879-888 (2021).
\bibitem{Zhang2020GLP1} Zhang X et al. Mol Cell 80, 485-500 (2020).
\bibitem{Shaye2020} Shaye H et al. Nature 584, 298-303 (2020).
\bibitem{Mao2020GABABR} Mao C et al. Cell Res 30, 564-573 (2020).
\bibitem{Berman2000} Berman HM et al. Nucleic Acids Res 28, 235-242 (2000).
\bibitem{Venkatakrishnan2013} Venkatakrishnan AJ et al. Nature 494, 185-194 (2013).
\bibitem{Do2023} Do HN et al. JACS Au 3, 3165-3180 (2023).
\bibitem{Jumper2021} Jumper J et al. Nature 596, 583-589 (2021).
\bibitem{Abramson2024} Abramson J et al. Nature 630, 493-500 (2024).
\bibitem{Wohlwend2024} Wohlwend J et al. bioRxiv 2024.11.19.624167 (2024).
\bibitem{Chai2024} Chai Discovery. bioRxiv 2024.10.10.615955 (2024).
\bibitem{Protenix2024} ByteDance. bioRxiv 2024 (2024).
\bibitem{HelixFold2024} Baidu. arXiv 2024 (2024).
\bibitem{Heo2022} Heo L, Feig M. Proteins 90, 1873-1885 (2022).
\bibitem{delAlamo2022} del Alamo D et al. eLife 11, e75751 (2022).
\bibitem{Corso2024} Corso G et al. J Chem Inf Model 62, 3142-3156 (2022).
\bibitem{Pandy2023} Pandy-Szekeres G et al. Nucleic Acids Res 51, D395-D402 (2023).
\bibitem{Stein2022} Stein RA et al. PLOS Comput Biol 18, e1010483 (2022).
\bibitem{WaymentSteele2024} Wayment-Steele HK et al. Nature 625, 832-839 (2024).
\bibitem{Sala2022} Sala D et al. J Chem Theory Comput 18, 7118-7126 (2022).
\bibitem{Lin2023} Lin Z et al. Science 379, 1123-1130 (2023).
\bibitem{Rives2021} Rives A et al. Proc Natl Acad Sci USA 118, e2016239118 (2021).
\bibitem{Satorras2021} Satorras VG et al. Proc ICML, 9323-9332 (2021).
\bibitem{Schutt2021} Schutt K et al. Proc ICML, 9377-9388 (2021).
\bibitem{Mao2025} Mao Z et al. J Chem Theory Comput 21, 7954-7966 (2025).
\bibitem{Wootten2018} Wootten D et al. Nat Rev Mol Cell Biol 19, 638-653 (2018).

\end{thebibliography}

\end{document}