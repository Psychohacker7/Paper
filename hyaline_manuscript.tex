%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hyaline: Deep Learning for GPCR Activation State Prediction
% Publication-Ready Manuscript
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  journal=largetwo,
  manuscript=article-type,
  year=2025,
  volume=1,
]{cup-journal}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage[nopatch]{microtype}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{subcaption}

% Custom commands
\newcommand{\hyaline}{\textsc{Hyaline}}
\newcommand{\gpcr}{GPCR}
\newcommand{\auroc}{AuROC}

\title{\hyaline: Geometric Deep Learning for Accurate Prediction of G Protein-Coupled Receptor Activation States from Structure}

\author{A. Khaleq, H. Kabodha}
\email[A. Khaleq]{ayman@varosync.com}
\email[Varosync]{partnerships@varosync.com}

\addbibresource{hyaline_references.bib}

\keywords{G protein-coupled receptors, deep learning, protein structure, activation state prediction, geometric neural networks, ESM3, drug discovery}

\begin{document}

%=======================================================================
% ABSTRACT
%=======================================================================
\begin{abstract}
Characterizing the conformational landscapes of G protein-coupled receptors (\textbf{GPCR}s) is fundamental to understanding signal transduction and accelerating rational drug design. Current computational approaches rely on static sequence analysis or lose critical geometric context, failing to resolve the structural switches that drive allosteric signaling. Here we introduce \textbf{Hyaline}, a geometric deep learning framework that leverages E(n)-equivariant graph neural networks and ESM3 evolutionary embeddings to predict \textbf{GPCR} activation states directly from 3D coordinates. By incorporating biological priors through motif-specific attention biasing, \textbf{Hyaline} achieves near-perfect classification (\textbf{AuROC} 0.995) on 1,590 experimental structures, significantly outperforming sequence-only models. \textbf{Hyaline} provides a rapid, interpretable framework for annotating receptor conformational states, establishing a foundation for high-throughput discovery of allosteric modulators.
\end{abstract}

%=======================================================================
% INTRODUCTION
%=======================================================================
\section{Introduction}

G protein-coupled receptors (\gpcr{}s) represent the largest family of druggable membrane proteins in the human genome, with approximately 34\% of FDA-approved drugs targeting this superfamily\autocite{Zhang2024,Hauser2021}. The therapeutic relevance of \gpcr{}s stems from their role as conformational switches: upon agonist binding, receptors transition from inactive to active states through conserved structural rearrangements (Fig.~\ref{fig:activation_overview}). Most prominently, the 6--14~\AA{} outward displacement of transmembrane helix 6 (TM6) opens the intracellular cavity for G protein engagement\autocite{Weis2018}.

%                 ---
% FIGURE 1: Activation Overview - HERO FIGURE
%                 ---
\begin{figure*}[!ht]
\centering
\includegraphics[width=0.85\textwidth]{"comparison_final(blue_2RH1_inactive, Tan_3SN6_active).png"}
\caption{\textbf{Structural basis of GPCR activation.} 
Superposition of the prototypical $\beta_2$-adrenergic receptor in inactive (blue; PDB: 2RH1) and active (tan; PDB: 3SN6) conformations. Activation involves a characteristic 14~\AA{} outward displacement of transmembrane helix 6 (TM6), opening the intracellular cavity to accommodate G protein binding. This conformational change is accompanied by rearrangements at conserved microswitches including the DRY motif (TM3), NPxxY motif (TM7), and CWxP rotamer toggle (TM6). \hyaline{} learns to detect these coordinated structural changes through geometric deep learning.}
\label{fig:activation_overview}
\end{figure*}

While cryo-electron microscopy has expanded the structural landscape to over 1,100 deposited \gpcr{} structures\autocite{Caniceiro2025}, determining the functional activation state remains a critical bottleneck. The heterogeneity of experimental conditions---varying ligand occupancy, stabilizing nanobodies, and crystallization constructs---frequently obscures whether a structure represents an active or inactive conformation. This annotation bottleneck impedes structure-based drug discovery, where distinguishing activation states is essential for designing state-selective modulators\autocite{Wootten2018,Kolb2022}.

Current computational approaches expose a fundamental limitation: the inability to bridge evolutionary sequence information with three-dimensional structural geometry. Protein language models such as ESM3 encode rich evolutionary priors but operate on sequence alone---they cannot distinguish conformations when the amino acid sequence is identical\autocite{Lin2023}. Structure prediction methods like AlphaFold generate static scaffolds that frequently represent ambiguous states\autocite{Abramson2024}. Hand-crafted structural features require class-specific parameterization and fail to generalize\autocite{Caniceiro2025}. Critically, standard neural networks lack geometric inductive biases to respect molecular symmetries. This \textit{sequence-structure gap} has prevented accurate, generalizable activation state prediction.

We hypothesized that geometric deep learning, which explicitly encodes E(n)-equivariant symmetries, could capture the non-local conformational rearrangements distinguishing \gpcr{} activation states. Here we introduce \hyaline{}, a framework unifying evolutionary and structural representations through: (1) ESM3 embeddings capturing evolutionary constraints; (2) E(n)-equivariant graph neural networks respecting molecular symmetries; and (3) motif-specific attention biasing incorporating biological priors. Validated using rigorous temporal splitting, \hyaline{} achieves \auroc{} 0.995 across all major \gpcr{} classes.

%=======================================================================
% RESULTS
%=======================================================================
\section{Results}

\subsection{Architecture integrates evolutionary and structural signals}

The central challenge in \gpcr{} activation state prediction is capturing coordinated conformational changes spanning 20--40~\AA{} while respecting physical symmetries. \hyaline{} addresses this through an architecture integrating evolutionary embeddings, geometric features, and biologically-motivated attention (Fig.~\ref{fig:architecture}).

%                 ---
% FIGURE 2: Architecture
%                 ---
\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/global_diagram.pdf}
\caption{\textbf{The Hyaline framework.} 
\textbf{(a)} Amino acid sequences are processed by ESM3 to generate 1,536-dimensional per-residue embeddings encoding evolutionary constraints. Structural geometry is encoded via RBF radius graphs (10~\AA{} cutoff) producing 96-dimensional edge features. 
\textbf{(b)} Embeddings and geometric features are fused within five E(n)-equivariant message-passing layers, ensuring predictions depend solely on relative atomic arrangements. 
\textbf{(c)} Soft motif-attention biasing guides the network to prioritize conserved microswitches while preserving flexibility to discover novel determinants.}
\label{fig:architecture}
\end{figure*}

We employ ESM3\autocite{Lin2023}, a 15-billion parameter transformer trained on billions of protein sequences, providing 1,536-dimensional per-residue embeddings encoding functional constraints shaped by evolution. However, these embeddings alone cannot distinguish activation states when identical sequences adopt different conformations.

To encode three-dimensional geometry, \hyaline{} represents each structure as a graph where nodes correspond to residues and edges connect spatially proximal pairs within 10~\AA{}. Pairwise C$_\alpha$ distances are encoded using 96 radial basis functions spanning 2--20~\AA{}, providing a smooth representation naturally invariant to rotations and translations.

The core innovation is E(n)-equivariant message passing\autocite{Satorras2021,Mao2025} (Fig.~\ref{fig:egnn}). Unlike standard graph neural networks, equivariant networks jointly update node features and coordinates respecting molecular symmetries: if input coordinates rotate by matrix $R$, coordinate updates also rotate by $R$, while scalar features remain unchanged. This ensures predictions depend only on relative atomic arrangements.

%                 ---
% FIGURE 3: EGNN Mechanism
%                 ---
\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/egnn_layer.pdf}
\caption{\textbf{E(n)-equivariant message passing.} 
Each layer simultaneously updates node coordinates $x_i$ and features $h_i$ through message functions depending on pairwise distances and RBF-encoded attributes, maintaining rotational equivariance by construction. The Motif Bias block injects learnable attention weights for conserved microswitches (DRY, NPxxY, CWxP), accelerating convergence without hard-coding classification rules.}
\label{fig:egnn}
\end{figure*}

Five message-passing layers provide sufficient receptive field to integrate information across the full transmembrane domain ($\sim$40~\AA{}). Based on structural studies\autocite{Weis2018,Hauser2021}, residues in the DRY, NPxxY, and CWxP motifs receive enhanced attention through a learned bias term ($b_{\text{motif}} = 0.5$), encouraging focus on activation-relevant regions while remaining free to discover additional features.

\subsection{Temporal validation demonstrates generalization}

To rigorously assess whether \hyaline{} learns generalizable features rather than memorizing training examples, we employed temporal validation: training on structures deposited before January 2023 ($n = 1,312$) and testing on 2023--2024 structures ($n = 278$). This ensures test structures were unavailable during model development.

%                 ---
% FIGURE 4: Performance
%                 ---
\begin{figure*}[!ht]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Benchmark_AComparisonClassC_GPCR_Overall.pdf}
    \caption{Method comparison}
    \label{fig:performance_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Benchmark_BResolution_Independence.pdf}
    \caption{Resolution independence}
    \label{fig:performance_b}
\end{subfigure}
\caption{\textbf{Generalization to temporally distinct structures.} 
\textbf{(a)} On the 2023--2024 test set, \hyaline{} substantially outperforms baselines. The 13.9 percentage point gap versus ESM3-only models demonstrates the critical contribution of geometric message passing. 
\textbf{(b)} Performance remains stable across resolutions from 1.5--4.0~\AA{}, indicating resolution-independent learned features. Error bars: 95\% CI via bootstrap.}
\label{fig:performance}
\end{figure*}

The temporal test set includes receptors poorly represented in training, structures from the latest cryo-EM methods, and complexes with novel ligands. \hyaline{} achieved \auroc{} 0.991 with minimal overfitting relative to cross-validation (Table~\ref{tab:performance}). Comparison against an ESM3-only baseline (Fig.~\ref{fig:performance}a) revealed a 13.9 percentage point performance gap, demonstrating that sequence-derived features alone are insufficient---geometric information is essential for capturing conformational differences. Performance remained robust across experimental resolutions (Fig.~\ref{fig:performance}b).

\begin{table}[!ht]
\centering
\caption{\textbf{Classification performance.}}
\label{tab:performance}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Cross-validation} & \textbf{Temporal Test} \\
\midrule
\auroc & $0.995 \pm 0.003$ & $0.991$ \\
Accuracy & $0.976 \pm 0.011$ & $0.964$ \\
Sensitivity & $0.990 \pm 0.008$ & $0.978$ \\
Specificity & $0.938 \pm 0.021$ & $0.921$ \\
F1 Score & $0.983 \pm 0.007$ & $0.970$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Attention mechanisms recover conserved activation switches}

A key advantage of \hyaline{} is interpretability through attention mechanisms. Analysis of learned attention weights revealed clear enrichment at conserved activation motifs (Fig.~\ref{fig:attention}).

%                 ---
% FIGURE 5: Attention
%                 ---
\begin{figure*}[!ht]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Motifattention_Amean.pdf}
    \caption{Mean attention by position}
    \label{fig:attention_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Motifattention_Bdistrib.pdf}
    \caption{Motif vs.\ variable regions}
    \label{fig:attention_b}
\end{subfigure}
\caption{\textbf{Autonomous recovery of activation switches.} 
\textbf{(a)} Mean attention weights show elevated importance at DRY (3.2$\times$), NPxxY (2.8$\times$), and CWxP (2.4$\times$) motifs, plus the intracellular end of TM5 (2.1$\times$) and helix 8 (1.9$\times$). 
\textbf{(b)} Attention distributions at conserved motifs versus variable regions ($p < 0.001$, permutation test) confirm the model prioritizes functionally critical positions.}
\label{fig:attention}
\end{figure*}

The DRY motif showed 3.2$\times$ enrichment, NPxxY 2.8$\times$, and CWxP 2.4$\times$ (all $p < 0.001$). Critically, this emerged despite weak initial biasing---the model amplified attention through learning. Beyond explicitly biased motifs, \hyaline{} autonomously identified additional activation elements: TM5 intracellular end (2.1$\times$), helix 8 (1.9$\times$), and ICL2 portions. Attention correlated strongly with TM6 displacement magnitude ($r = 0.78$, $p < 0.001$), confirming the model learned to focus on the primary activation signature without explicit training.

\subsection{Generalization across receptor families}

To assess cross-family generalization, we stratified performance by receptor class (Fig.~\ref{fig:families}, Table~\ref{tab:family}).

%                 ---
% FIGURE 6: Family Performance
%                 ---
\begin{figure*}[!ht]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Embedding_Alearned.pdf}
    \caption{Activation state clustering}
    \label{fig:families_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Embeddings_BGPCRclass.pdf}
    \caption{GPCR class clustering}
    \label{fig:families_b}
\end{subfigure}
\caption{\textbf{Learned representations encode a universal activation axis.} 
t-SNE of 320-dimensional graph representations ($n = 278$). 
\textbf{(a)} Structures cluster primarily by activation state, demonstrating a generalizable ``activation axis'' across families. 
\textbf{(b)} Secondary clustering by receptor class reflects family-specific features superimposed on the shared activation signature.}
\label{fig:families}
\end{figure*}

\begin{table}[!ht]
\centering
\caption{\textbf{Performance by receptor family.}}
\label{tab:family}
\begin{tabular}{lccccc}
\toprule
\textbf{Class} & $\boldsymbol{n}$ & \textbf{\auroc} & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{F1} \\
\midrule
Class A & 1,263 & 0.997 & 98.3\% & 99.2\% & 0.988 \\
Class B1 & 145 & 0.988 & 95.9\% & 97.1\% & 0.968 \\
Class C & 94 & 0.971 & 91.5\% & 93.2\% & 0.938 \\
Class F & 34 & 0.956 & 88.2\% & 90.0\% & 0.900 \\
\bottomrule
\end{tabular}
\end{table}

Class A receptors, sharing the canonical ionic lock mechanism, achieved \auroc{} 0.997. Remarkably, performance remained strong for classes with fundamentally different mechanisms. Class B1 receptors exhibit a sharp TM6 kink rather than rigid-body rotation\autocite{Zhang2017GLP1} (\auroc{} 0.988). Class C receptors function as obligate dimers with subtle TM movements ($\sim$2--4~\AA{} vs.\ 6--14~\AA{} in Class A)\autocite{Ellaithy2020} (\auroc{} 0.971). Even Class F achieved \auroc{} 0.956 despite only 34 structures.

Latent representations revealed hierarchical organization (Fig.~\ref{fig:families}): structures cluster primarily by activation state (a universal ``activation axis''), with secondary clustering by receptor class capturing family-specific features.

\subsection{Ablation studies quantify architectural contributions}

Systematic ablations revealed both evolutionary embeddings and geometric message passing are essential (Fig.~\ref{fig:ablation}).

%                 ---
% FIGURE 7: Ablation
%                 ---
\begin{figure}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/Architecture_Ablation_Study.pdf}
\caption{\textbf{Ablation studies.} 
Removing geometric encoding produces the largest decline ($\Delta$\auroc{} = $-$17.2\%), confirming 3D structure drives state discrimination. Motif biasing ($-$2.7\%), RBF encoding ($-$2.4\%), and message-passing depth ($-$1.4\%) each contribute incrementally.}
\label{fig:ablation}
\end{figure}

Replacing ESM3 with one-hot encodings reduced \auroc{} by 17.2 percentage points, confirming the value of evolutionary pretraining. RBF distance encoding contributed 2.4\%, motif biasing 2.7\%, and increasing message-passing layers from 3 to 5 added 1.4\%. Notably, unbiased models still learned to attend to key motifs, suggesting biasing accelerates learning rather than providing otherwise unavailable information.

\subsection{Error analysis reveals biological ambiguity}

Examination of the 39 misclassified structures revealed errors predominantly reflect genuine biological ambiguity (Fig.~\ref{fig:ionic_lock}).

%                 ---
% FIGURE 8: Ionic Lock Detail
%                 ---
\begin{figure}[!ht]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/2RH1_dry_closeup.png}
    \caption{Inactive: ionic lock intact (3.4~\AA{})}
    \label{fig:ionic_lock_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/3SN6_dry_closeup.png}
    \caption{Active: ionic lock broken (11.2~\AA{})}
    \label{fig:ionic_lock_b}
\end{subfigure}
\caption{\textbf{Molecular basis of state discrimination.} 
Closeup of the DRY motif in $\beta_2$-adrenergic receptor. 
\textbf{(a)} Inactive state: R$^{3.50}$--E$^{6.30}$ salt bridge stabilizes closed conformation. 
\textbf{(b)} Active state: ionic lock broken as TM6 moves outward. \hyaline{} attention concentrates on this region, confirming learned features match known activation physics.}
\label{fig:ionic_lock}
\end{figure}

\textbf{Partially activated intermediates} (38\% of errors): Structures annotated ``active'' based on bound agonist but lacking G protein stabilization. TM6 displacement is incomplete; \hyaline{} correctly identifies these as geometrically closer to inactive.

\textbf{Class C receptors} (28\%): Structures near the decision boundary reflecting fundamentally different activation mechanisms with subtle TM movements.

\textbf{Experimental artifacts} (21\%): Missing loops, stabilizing mutations, or crystal packing constraints.

\textbf{Unexplained} (13\%): Possible annotation errors or genuinely ambiguous conformations.

%=======================================================================
% DISCUSSION
%=======================================================================
\section{Discussion}

\hyaline{} achieves near-perfect accuracy in predicting \gpcr{} activation states by unifying evolutionary embeddings with E(n)-equivariant graph neural networks, bridging the sequence-structure gap limiting previous approaches.

The exceptional performance stems from three architectural choices. ESM3 embeddings provide evolutionary context essential for distinguishing functionally important changes from neutral variation. The equivariant architecture ensures predictions depend only on relative atomic arrangements---outperforming rotation augmentation. Soft motif biasing accelerates learning while remaining flexible; unbiased models still learn to attend to key motifs, confirming these are genuinely important.

Our error analysis reveals most misclassifications reflect genuine biological ambiguity. Partially activated intermediates---agonist-bound receptors without transducer stabilization---represent meta-stable intermediates along the activation pathway\autocite{Weis2018}. \hyaline{}'s intermediate confidence scores may be more informative than binary annotations. These states represent opportunities for ``biased agonists'' stabilizing specific pathway points\autocite{Wootten2018,Kolb2022}.

Perhaps the most significant application is assessment of AI-predicted structures. AlphaFold 3, Boltz-1, and Chai-1 frequently produce ambiguous conformational states\autocite{Abramson2024,Xu2025}. \hyaline{} provides orthogonal assessment enabling informed selection for virtual screening.

The principles underlying \hyaline{}---unifying evolutionary and structural representations through equivariant architectures---apply broadly to kinases, ion channels, and other proteins undergoing conformational transitions. As structural databases expand, automated annotation becomes increasingly important. \hyaline{} establishes a foundation for computational characterization of protein functional states at scale.

%=======================================================================
% METHODS
%=======================================================================
\section{Methods}

\subsection{Dataset}
\gpcr{} structures from PDB (December 2024) cross-referenced with GPCRdb\autocite{Kooistra2021}. Inclusion: $\geq$80\% transmembrane domain resolved, resolution $\leq$4.0~\AA{} (X-ray) or FSC $\leq$4.5~\AA{} (cryo-EM), unambiguous activation annotation. Temporal split: training (pre-2023; $n = 1,312$), test (2023--2024; $n = 278$).

\subsection{Features}
ESM3-Open embeddings (1,536-dim per residue)\autocite{Lin2023}. Graph construction: edges within 10~\AA{} C$_\alpha$ distance. RBF edge features: 96 Gaussian expansions (2--20~\AA{}, $\sigma = 0.3$~\AA{}).

%                 ---
% FIGURE 9: Graph Construction
%                 ---
\begin{figure*}[!ht]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Geometric_ARBF_Features_DRY_Motif.pdf}
    \caption{RBF distance encoding}
    \label{fig:graph_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Geometric_BMotif_attention_bias.pdf}
    \caption{Motif attention biasing}
    \label{fig:graph_b}
\end{subfigure}
\caption{\textbf{Graph construction and feature encoding.} 
\textbf{(a)} RBF encoding enables learning distance-dependent interaction patterns. 
\textbf{(b)} Conserved microswitches receive additive attention bias ($b_{\text{motif}} = 0.5$).}
\label{fig:graph}
\end{figure*}

\subsection{Architecture}
Five EGNN layers\autocite{Satorras2021} with hidden dimension 320, SiLU activations, pre-layer normalization, dropout 0.1. Global attention pooling to 320-dim graph representation. Two-layer MLP classifier with dropout 0.2.

\subsection{Training}
Binary cross-entropy with class weighting (0.38:1.0). AdamW optimizer (lr $3 \times 10^{-4}$, weight decay $10^{-4}$). Cosine annealing with 5-epoch warmup, 30 epochs, early stopping (patience 5). Gaussian coordinate noise ($\sigma = 0.1$~\AA{}). Cross-validation: 30\% sequence identity clustering (MMseqs2).

\subsection{Implementation}
PyTorch 2.1.2, PyTorch Geometric 2.4.0. Training: NVIDIA A100, $\sim$4 hours (5-fold CV). Inference: 0.5s/structure, $\sim$7,200/hour throughput. Code: \url{https://github.com/Varosync/Hyaline}.

%=======================================================================
% DECLARATIONS
%=======================================================================
\paragraph{Competing Interests}
Research conducted as part of Varosync's R\&D activities.

\paragraph{Data Availability}
Structures from PDB. Dataset, models, code at \url{https://github.com/Varosync/Hyaline}.

\clearpage
\section*{Extended Data}

\subsection*{Extended Data Figure 1: Computational Efficiency}

\begin{figure*}[!ht]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Scalability_AFunctional_geometry_JKstates.pdf}
    \caption{Jumping Knowledge preserves geometry}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Scalability_Bruntime.pdf}
    \caption{Linear runtime scaling}
\end{subfigure}
\caption{\textbf{Scalability.} Linear $O(N)$ complexity enables high-throughput application.}
\label{fig:ext_scale}
\end{figure*}

\subsection*{Extended Data Figure 2: Readout Architecture}

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/grphread_classif.pdf}
\caption{\textbf{Jumping Knowledge aggregation.} Representations from all layers concatenated prior to pooling, capturing both local and long-range features.}
\label{fig:ext_readout}
\end{figure*}

\subsection*{Extended Data Table 1: Hyperparameters}

\begin{table}[!ht]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
ESM3 embedding dim & 1,536 \\
Hidden dimension & 320 \\
Message passing layers & 5 \\
RBF centers & 96 (2--20 \AA) \\
Edge cutoff & 10 \AA \\
Motif bias & 0.5 \\
Learning rate & $3 \times 10^{-4}$ \\
Batch size & 32 \\
\bottomrule
\end{tabular}
\end{table}

%=======================================================================
% BIBLIOGRAPHY
%=======================================================================
\printendnotes
\printbibliography

\end{document}
