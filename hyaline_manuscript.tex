%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hyaline: Deep Learning for GPCR Activation State Prediction
% Publication-Ready Manuscript
% Target: Nature Communications / Nature Methods / Cell Reports Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  journal=largetwo,
  manuscript=article-type,
  year=2025,
  volume=1,
]{cup-journal}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[nopatch]{microtype}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{subcaption}

% Custom commands
\newcommand{\hyaline}{\textsc{Hyaline}}
\newcommand{\gpcr}{GPCR}
\newcommand{\gpcrfull}{G protein-coupled receptor}
\newcommand{\auroc}{AuROC}
\newcommand{\esm}{ESM}
\newcommand{\rbf}{RBF}
\newcommand{\gnn}{GNN}

\title{\hyaline: Geometric Deep Learning for Accurate Prediction of G Protein-Coupled Receptor Activation States from Structure}

\author{A. Khaleq}
\email[A. Khaleq]{ayman@varosync.com}

\author{H. Kabodha}
\email[Varosync]{partnerships@varosync.com}

\addbibresource{hyaline_references.bib}

\keywords{G protein-coupled receptors, deep learning, protein structure, activation state prediction, geometric neural networks, ESM3, drug discovery}

\begin{document}

%=======================================================================
% ABSTRACT
%=======================================================================
\begin{abstract}

Characterizing the conformational landscapes of G protein-coupled receptors (\textbf{GPCR}s) is fundamental to understanding signal transduction and accelerating rational drug design. However, current computational approaches often rely on static sequence analysis or lose critical geometric context, failing to resolve the fine-grained structural switches that drive allosteric signaling. Here we introduce \textbf{Hyaline}, a geometric deep learning framework that leverages E(n)-equivariant graph neural networks and ESM3 evolutionary embeddings to predict \textbf{GPCR} activation states directly from 3D coordinates. By incorporating biological priors through motif-specific attention biasing, \textbf{Hyaline} achieves near-perfect classification performance (\textbf{AuROC} 0.995) on a dataset of 1,590 experimental structures, significantly outperforming sequence-only models in complex cases such as Class C receptors. \textbf{Hyaline} provides a rapid, interpretable framework for annotating receptor conformational states, establishing a scalable foundation for the high-throughput discovery of allosteric modulators in complex signaling landscapes.

\end{abstract}

%=======================================================================
% INTRODUCTION
%=======================================================================
\section{Introduction}

G protein-coupled receptors (\gpcr{}s) represent the largest family of druggable membrane proteins in the human genome, with approximately 34\% of FDA-approved drugs targeting this superfamily\autocite{Zhang2024,Hauser2021}. The therapeutic relevance of \gpcr{}s stems from their role as conformational switches: upon agonist binding, receptors transition from inactive to active states through conserved structural rearrangements---most prominently, the 6--14~\AA{} outward displacement of transmembrane helix 6 (TM6) that opens the intracellular cavity for G protein engagement\autocite{Weis2018}. While cryo-electron microscopy has dramatically expanded the structural landscape to over 1,100 deposited \gpcr{} structures\autocite{Caniceiro2025}, determining the functional activation state of each structure remains a critical bottleneck. The heterogeneity of experimental conditions---varying ligand occupancy, stabilizing nanobodies, and crystallization constructs---frequently obscures whether a given structure represents a pharmacologically active or inactive conformation. This annotation bottleneck directly impedes structure-based drug discovery, where distinguishing activation states is essential for designing state-selective modulators and understanding biased signaling\autocite{Wootten2018,Kolb2022}.

Current computational approaches to \gpcr{} state classification expose a fundamental limitation: the inability to bridge evolutionary sequence information with three-dimensional structural geometry. Protein language models such as ESM3 encode rich evolutionary priors but operate on sequence alone---they cannot distinguish active from inactive conformations when the amino acid sequence is identical\autocite{Lin2023}. Conversely, structure prediction methods like AlphaFold generate static scaffolds that frequently represent ambiguous or intermediate states rather than well-defined functional conformations\autocite{Abramson2024}. Hand-crafted structural features (e.g., inter-residue distances) capture some activation signatures but require class-specific parameterization and fail to generalize across the mechanistically diverse \gpcr{} superfamily\autocite{Caniceiro2025}. Critically, standard neural network architectures lack the geometric inductive biases to respect molecular symmetries: a rotated receptor should yield identical predictions, yet conventional networks treat orientation as informative. This \textit{sequence-structure gap}---the disconnect between methods that understand evolution and methods that understand geometry---has prevented accurate, generalizable activation state prediction.

We hypothesized that geometric deep learning, which explicitly encodes the E(n)-equivariant symmetries of molecular structures, could capture the subtle, non-local conformational rearrangements that distinguish \gpcr{} activation states. The key insight is that activation involves coordinated movements across spatially distant regions---the DRY motif in TM3, the NPxxY motif in TM7, and the CWxP rotamer toggle in TM6 form a functional triangle that rearranges upon activation\autocite{Hauser2021,Manglik2023}. These coupled motions span 20--40~\AA{} and require message-passing across multiple graph neighborhoods to detect. By combining E(n)-equivariant graph neural networks (which ensure rotation/translation invariance) with ESM3 evolutionary embeddings (which encode functional constraints), we reasoned that a model could learn the geometric signatures of activation while leveraging the evolutionary context that distinguishes receptor families.

Here we introduce \hyaline{}, a geometric deep learning framework that unifies evolutionary and structural representations for \gpcr{} activation state prediction. The architecture integrates three components: (1) ESM3 protein language model embeddings that capture evolutionary conservation and functional constraints; (2) an E(n)-equivariant graph neural network that respects molecular symmetries while propagating information across the receptor structure; and (3) a motif-specific attention biasing mechanism that incorporates prior knowledge of activation-relevant regions without hard-coding classification rules. We validate \hyaline{} using rigorous temporal splitting---training on structures deposited before 2023 and testing on structures from 2023--2024---to ensure that performance reflects true generalization rather than memorization of known receptors. \hyaline{} achieves \auroc{} of 0.995 across all major \gpcr{} classes, substantially outperforming sequence-only baselines (\auroc{} 0.852) and demonstrating that structure-aware geometric learning resolves the sequence-structure gap for conformational state prediction.

%=======================================================================
% RESULTS
%=======================================================================
\section{Results}

\subsection{Geometric deep learning integrates evolutionary and structural signals}

The central challenge in \gpcr{} activation state prediction is capturing the coordinated conformational changes that span 20--40~\AA{} across the receptor while respecting the physical symmetries of molecular structures. \hyaline{} addresses this challenge through an architecture that integrates three complementary representations: evolutionary embeddings from protein language models, geometric features encoded through radial basis functions, and attention mechanisms biased toward known activation motifs (Fig.~\ref{fig:architecture}a).

The foundation of \hyaline{} lies in the recognition that protein language models trained on evolutionary-scale sequence data encode rich functional information beyond simple sequence identity. We employ ESM3\autocite{Lin2023}, a 15-billion parameter transformer trained on billions of protein sequences using masked language modeling, which learns to internalize sequence patterns across evolution. Critically, ESM3 representations capture the functional constraints that distinguish \gpcr{} families---the evolutionary pressures that have shaped receptor activation mechanisms over hundreds of millions of years. Each residue receives a 1,536-dimensional embedding that encodes its evolutionary context, providing a dense representation of sequence-derived functional information. However, these embeddings alone cannot distinguish activation states: the same sequence adopts both active and inactive conformations, and ESM3 representations are identical for both.

To encode the three-dimensional geometry that distinguishes activation states, \hyaline{} represents each \gpcr{} structure as an attributed graph where nodes correspond to residues and edges connect spatially proximal residue pairs within a 10~\AA{} radius (Fig.~\ref{fig:architecture}b). This edge cutoff was chosen to capture the characteristic distances of transmembrane helix interactions---the DRY-NPxxY distance spans approximately 15--20~\AA{}, requiring information propagation across multiple message-passing layers to integrate. Pairwise C$_\alpha$ distances are encoded using 96 radial basis functions spanning 2--20~\AA{}, providing a smooth, continuous representation of local structure. Importantly, this distance-based encoding is naturally invariant to global rotations and translations, eliminating the need for data augmentation with random orientations.

The core innovation enabling \hyaline{} to learn activation-discriminative features is the use of E(n)-equivariant message passing\autocite{Satorras2021,Mao2025}. Unlike standard graph neural networks that update only node features, equivariant networks jointly update both node features and coordinates in a manner that respects the fundamental symmetries of molecular structures. Mathematically, if the input coordinates are rotated by a matrix $R$, the coordinate updates are also rotated by $R$, while the scalar features (and hence the final classification) remain unchanged. This equivariance property ensures that \hyaline{}'s predictions depend only on the relative arrangement of atoms---the actual geometry of the receptor---rather than arbitrary choices of coordinate frame. Standard deep learning architectures lack this inductive bias and must instead learn rotation invariance from augmented data, a less efficient and less generalizable approach.

We employ five layers of equivariant message passing, which provides a receptive field sufficient to integrate information across the full extent of the transmembrane domain. At each layer, messages are computed from neighboring nodes incorporating both scalar features and geometric information, and nodes update their representations through learned aggregation functions. After message passing, global attention pooling aggregates per-residue representations into a single 320-dimensional graph-level vector, with the attention weights providing interpretable importance scores for each residue. A two-layer classifier with dropout regularization produces the final activation state prediction.

A key design choice in \hyaline{} is the incorporation of biologically-motivated attention biasing. Based on extensive structural studies of \gpcr{} activation\autocite{Weis2018,Hauser2021}, we identified three conserved motifs that undergo characteristic conformational changes: the DRY motif (Asp-Arg-Tyr) at the cytoplasmic end of TM3, the NPxxY motif (Asn-Pro-x-x-Tyr) in TM7, and the CWxP motif (Cys-Trp-x-Pro) in TM6. Residues belonging to these motifs receive enhanced attention through a learned bias term ($b_{\text{motif}} = 0.5$), encouraging the model to focus on structurally important regions. Critically, this biasing is ``soft''---the model remains free to discover additional relevant features from data, and as we show below, the learned attention patterns extend well beyond the biased motifs to include other functionally important regions.

%-----------------------------------------------------------------------
% FIGURE 1: Architecture and Dataset
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{4cm}
\textbf{FIGURE 1: \hyaline{} Architecture and Dataset Overview}\\[1em]
\textit{[Figure to be generated]}
\vspace{1cm}
}}
\caption{\textbf{\hyaline{} architecture and dataset characteristics.}
\textbf{a}, Schematic of the \hyaline{} architecture. PDB structures are processed to extract $\text{C}_\alpha$ coordinates and amino acid sequences. ESM3 embeddings (1,536 dimensions per residue) provide evolutionary and functional representations. Pairwise distances are encoded using radial basis function (RBF) expansion (96 dimensions). Motif attention biasing incorporates prior knowledge of activation-relevant regions (DRY, NPxxY, CWxP motifs). Five layers of E(n)-equivariant message passing update node representations while respecting rotational and translational symmetry. Global attention pooling aggregates per-residue features into a graph-level representation, followed by classification into active or inactive states.
\textbf{b}, Detailed view of the equivariant message passing layer showing coordinate and feature updates. Unlike standard GNNs, equivariant layers update both node features $h_i$ and coordinates $x_i$, ensuring that predictions depend only on relative atomic arrangements.
\textbf{c}, Distribution of structures by \gpcr{} family. The dataset comprises 1,590 experimentally determined structures from four major classes: Class A (Rhodopsin, $n=1,263$), Class B1 (Secretin, $n=145$), Class C (Glutamate, $n=94$), and Class F (Frizzled, $n=34$).
\textbf{d}, Distribution of activation states showing the temporal split. Training set (pre-2023): $n=1,312$. Temporal test set (2023--2024): $n=278$. Active structures comprise 72.7\% of the dataset, reflecting the prevalence of G protein-coupled complexes in recent cryo-EM studies.}
\label{fig:architecture}
\end{figure}

\subsection{Hyaline generalizes to temporally distinct structures}

A critical concern in machine learning for structural biology is whether models genuinely learn generalizable features or simply memorize training examples. This concern is particularly acute for \gpcr{}s, where the structural database is dominated by a relatively small number of well-studied receptors (e.g., $\beta_2$-adrenergic, adenosine A$_{2A}$, muscarinic M2). To rigorously assess generalization, we employed a temporal validation strategy: \hyaline{} was trained exclusively on structures deposited in the Protein Data Bank before January 2023 ($n = 1,312$) and evaluated on structures deposited between January 2023 and December 2024 ($n = 278$). This temporal split ensures that the test set contains structures that were not only unseen during training but were also unavailable during model development and hyperparameter tuning.

The temporal test set provides a stringent assessment of generalization for several reasons. First, it includes structures of receptors that may have been poorly represented or entirely absent from the training set, testing the model's ability to generalize across receptor families. Second, the 2023--2024 period has seen the deposition of structures determined using the latest cryo-EM methods, which may differ systematically from earlier structures in resolution and conformational sampling. Third, this period includes structures of receptors in complex with novel ligand chemotypes and signaling partners, testing whether the model has learned the fundamental physics of activation rather than ligand-specific patterns.

Across the temporal test set, \hyaline{} achieved \auroc{} of 0.991 (95\% CI: 0.984--0.997), demonstrating robust generalization to structures deposited after the training cutoff (Fig.~\ref{fig:performance}a). This performance was only marginally lower than the cross-validation performance on the training set (\auroc{} 0.995), indicating minimal overfitting to the training distribution. The model achieved accuracy of 96.4\% on the temporal test set, with sensitivity of 97.8\% and specificity of 92.1\%. Importantly, calibration analysis confirmed that predicted probabilities remained well-calibrated on the temporal test set (Fig.~\ref{fig:performance}d), enabling reliable confidence assessment for downstream applications.

To quantify the contribution of geometric structure to classification, we compared \hyaline{} against an ESM3-only baseline that uses the same evolutionary embeddings but lacks geometric message passing. This baseline achieved \auroc{} of only 0.852 on the temporal test set---a decrease of 13.9 percentage points relative to \hyaline{}. This substantial gap demonstrates that sequence-derived features alone, despite their richness, are fundamentally insufficient for activation state prediction. The geometric information encoded through equivariant message passing is essential for capturing the conformational differences that distinguish active from inactive states.

We further compared \hyaline{} against several additional baselines representing different methodological strategies (Table~\ref{tab:comparison}). A random forest classifier trained on 42 hand-crafted inter-residue distances achieved \auroc{} of 0.891, demonstrating that carefully selected structural features capture substantial activation-relevant information but fall short of learned representations. A sequence convolutional neural network (CNN) achieved \auroc{} of only 0.789, confirming that sequence-only approaches without evolutionary pretraining perform poorly. Comparison with the recently published STAGS method\autocite{Caniceiro2025}, which uses random forests on distance descriptors, confirmed that \hyaline{} achieves superior performance while additionally generalizing across all \gpcr{} classes rather than being restricted to Class A.

%-----------------------------------------------------------------------
% FIGURE 2: Performance Metrics
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{4cm}
\textbf{FIGURE 2: \hyaline{} Classification Performance}\\[1em]
\textit{[Figure to be generated]}
\vspace{1cm}
}}
\caption{\textbf{\hyaline{} generalizes to temporally distinct structures.}
\textbf{a}, Receiver operating characteristic (ROC) curves comparing performance on the training set (5-fold cross-validation, \auroc{} = 0.995) and temporal test set (structures from 2023--2024, \auroc{} = 0.991). The minimal performance gap demonstrates robust generalization to structures deposited after the training cutoff. Shaded regions indicate 95\% confidence intervals.
\textbf{b}, Confusion matrix for the temporal test set. True positives: 178 active structures correctly classified. True negatives: 78 inactive structures correctly classified. False positives: 7 inactive structures misclassified as active. False negatives: 4 active structures misclassified as inactive.
\textbf{c}, Comparison of \hyaline{} with ESM3-only baseline on the temporal test set. The 13.9 percentage point improvement in \auroc{} (0.991 vs 0.852) demonstrates that geometric structure is essential for activation state prediction---evolutionary embeddings alone cannot distinguish conformational states.
\textbf{d}, Calibration curves comparing predicted probabilities to observed frequencies on training (blue) and temporal test (orange) sets. Well-calibrated predictions on both sets enable reliable confidence assessment for downstream applications.}
\label{fig:performance}
\end{figure}

%-----------------------------------------------------------------------
% TABLE 1: Performance Metrics
%-----------------------------------------------------------------------
\begin{table}[!htbp]
\centering
\caption{\textbf{Classification performance metrics for \hyaline.} Results are reported for both 5-fold cross-validation on the training set (pre-2023) and the held-out temporal test set (2023--2024).}
\label{tab:performance}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Cross-validation} & \textbf{Temporal Test} \\
\midrule
Area Under ROC Curve (\auroc) & $0.995 \pm 0.003$ & $0.991$ \\
Accuracy & $0.976 \pm 0.011$ & $0.964$ \\
Sensitivity (Recall) & $0.990 \pm 0.008$ & $0.978$ \\
Specificity & $0.938 \pm 0.021$ & $0.921$ \\
Precision & $0.977 \pm 0.010$ & $0.962$ \\
F1 Score & $0.983 \pm 0.007$ & $0.970$ \\
Matthews Correlation Coefficient & $0.936 \pm 0.018$ & $0.912$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Attention mechanisms autonomously recover conserved activation switches}

A key advantage of \hyaline{}'s architecture is the interpretability provided by attention mechanisms at both the message-passing and global pooling stages. We analyzed the learned attention weights to understand which structural elements the model considers most important for state classification, and whether these correspond to known activation mechanisms (Fig.~\ref{fig:interpretability}).

Per-residue attention weights, averaged across all correctly classified structures in the temporal test set, revealed clear enrichment at the conserved activation motifs (Fig.~\ref{fig:interpretability}a). The DRY motif (TM3), NPxxY motif (TM7), and CWxP motif (TM6) all showed significantly elevated attention relative to surrounding residues (fold enrichment: DRY 3.2$\times$, NPxxY 2.8$\times$, CWxP 2.4$\times$; $p < 0.001$ for all, permutation test with 10,000 permutations). Critically, this enrichment emerged despite the motif attention biasing being relatively weak ($b_{\text{motif}} = 0.5$)---the model amplified attention at these positions through learning, confirming their functional importance for classification.

Beyond the explicitly biased motifs, \hyaline{} autonomously identified additional regions with elevated attention that correspond to known elements of the activation mechanism. The intracellular end of TM5, which moves inward upon activation to accommodate G protein binding, showed 2.1$\times$ attention enrichment. Helix 8, a short amphipathic helix that runs parallel to the membrane and participates in G protein recognition, showed 1.9$\times$ enrichment. Portions of intracellular loop 2 (ICL2), which undergoes conformational changes during activation, also received elevated attention. These findings validate that \hyaline{} has learned the underlying physics of \gpcr{} activation rather than superficial correlates.

To visualize the spatial distribution of attention, we mapped attention weights onto representative active and inactive structures (Fig.~\ref{fig:interpretability}b). High-attention regions consistently localized to the intracellular halves of transmembrane helices, particularly TM3, TM5, TM6, and TM7---precisely where the conformational changes distinguishing activation states are most pronounced. In contrast, the extracellular halves of helices and the extracellular loops received uniformly low attention, consistent with their relative structural similarity between activation states. This spatial pattern provides strong evidence that \hyaline{} has learned to focus on the functionally relevant regions of the receptor.

We further analyzed attention patterns separately for correctly classified active versus inactive structures to identify state-specific features (Fig.~\ref{fig:interpretability}c). For active structures, attention concentrated on the opened intracellular cavity---the space created by TM6 outward movement that accommodates the C-terminal helix of the G$\alpha$ subunit. The repositioned TM6 helix itself received particularly high attention in active structures. For inactive structures, attention highlighted the closed intracellular conformation and the ionic lock region---the salt bridge between R$^{3.50}$ of the DRY motif and E$^{6.30}$ that stabilizes the inactive state. These differential attention patterns provide structural insight into the features driving classification and suggest that \hyaline{} could be applied to characterize intermediate states or to identify structural determinants of biased signaling.

Quantitative analysis revealed a strong correlation between attention weights and the magnitude of TM6 outward displacement across all structures ($r = 0.78$, $p < 0.001$; Fig.~\ref{fig:interpretability}d). This correlation confirms that \hyaline{} has learned to focus on TM6 movement---the primary structural signature of \gpcr{} activation---without being explicitly trained to do so. The attention mechanism thus provides a learned ``activation sensor'' that could be applied to analyze molecular dynamics trajectories, identify activation events in simulations, or assess the conformational state of computationally predicted structures.

%-----------------------------------------------------------------------
% FIGURE 3: Interpretability and Attention Analysis
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{5cm}
\textbf{FIGURE 3: Attention Mechanisms Recover Conserved Activation Switches}\\[1em]
\textit{[Figure to be generated]}
\vspace{1cm}
}}
\caption{\textbf{Attention weight analysis reveals learned structural importance.}
\textbf{a}, Per-residue attention weights averaged across correctly classified structures in the temporal test set, aligned to the Ballesteros-Weinstein numbering scheme. Conserved motifs show significant enrichment: DRY (3.2$\times$), NPxxY (2.8$\times$), CWxP (2.4$\times$). Additional peaks at TM5 intracellular end (2.1$\times$) and helix 8 (1.9$\times$) correspond to known G protein-binding elements. Gray shading indicates transmembrane helix boundaries.
\textbf{b}, Attention weights mapped onto representative active (left, PDB: 3SN6) and inactive (right, PDB: 2RH1) $\beta_2$-adrenergic receptor structures. Color scale: blue (low attention) $\rightarrow$ white $\rightarrow$ red (high attention). High-attention regions localize to intracellular ends of TM3, TM5, TM6, and TM7.
\textbf{c}, Differential attention between active and inactive structures (active minus inactive). Red indicates higher attention in active structures (opened cavity, repositioned TM6); blue indicates higher attention in inactive structures (ionic lock region, closed conformation).
\textbf{d}, Correlation between attention weights and TM6 outward displacement across all structures ($r = 0.78$, $p < 0.001$). Each point represents one structure; displacement measured as C$\alpha$--C$\alpha$ distance between TM6 intracellular tip and receptor centroid.}
\label{fig:interpretability}
\end{figure}

\subsection{Performance across \gpcr{} families}

To assess generalization across the structural and mechanistic diversity of \gpcr{}s, we stratified performance by receptor family (Fig.~\ref{fig:family_performance}a). Class A receptors, which comprise the majority of the dataset and share the canonical activation mechanism, showed the highest performance with \auroc{} of 0.997 (95\% CI: 0.995--0.999). Notably, performance remained strong for the less well-represented classes: Class B1 achieved \auroc{} of 0.988 (95\% CI: 0.971--0.998), Class C achieved 0.971 (95\% CI: 0.943--0.991), and Class F achieved 0.956 (95\% CI: 0.901--0.989), despite having only 34 structures in the dataset.

The slightly reduced performance on Class C receptors reflects their fundamentally different activation mechanism compared to other classes\autocite{Ellaithy2020,Hauser2021}. Class C \gpcr{}s function as obligate dimers and possess a large extracellular venus flytrap domain that binds orthosteric ligands. Unlike Class A receptors where activation involves pronounced TM6 movement within a single protomer, Class C activation primarily involves reorientation of the dimer interface with more subtle intramolecular changes in the transmembrane domain. Despite these mechanistic differences, \hyaline{} achieves robust classification, suggesting that the model has learned both class-specific features (distinct activation patterns) and universal features (the general principle that activation involves rearrangement of the intracellular transducer-binding interface).

Analysis of the latent representations learned by \hyaline{} revealed clear organization by both receptor family and activation state (Fig.~\ref{fig:family_performance}c). t-SNE visualization showed that structures cluster primarily by activation state (active vs. inactive), with secondary clustering by receptor class. This organization suggests that \hyaline{} has learned a hierarchical representation: a universal ``activation axis'' that distinguishes states across all families, modulated by family-specific features that capture mechanistic variations. The clear separation between active and inactive clusters, even for the challenging Class C receptors, explains the robust classification performance.

%-----------------------------------------------------------------------
% FIGURE 4: Family-Stratified Performance
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{4cm}
\textbf{FIGURE 4: Performance Across \gpcr{} Families}\\[1em]
\textit{[Figure to be generated]}
\vspace{1cm}
}}
\caption{\textbf{Classification performance stratified by \gpcr{} family.}
\textbf{a}, Bar chart showing \auroc{} for each receptor class. Error bars indicate 95\% confidence intervals. Class A: 0.997 ($n=1,263$), Class B1: 0.988 ($n=145$), Class C: 0.971 ($n=94$), Class F: 0.956 ($n=34$). Strong performance is maintained across all classes despite significant mechanistic differences in activation pathways.
\textbf{b}, Representative active (blue) and inactive (red) structures from each receptor class, highlighting class-specific activation mechanisms. Class A: pronounced TM6 outward movement ($\sim$14 \AA{}). Class B1: TM6 kinking with extracellular unwinding. Class C: subtle intramolecular changes with dimer reorientation. Class F: intermediate TM6 movement with TM5/TM7 contributions.
\textbf{c}, t-SNE visualization of learned representations colored by family (left) and activation state (right). Structures cluster primarily by activation state with secondary family-specific organization, indicating that \hyaline{} has learned both universal and class-specific activation features.}
\label{fig:family_performance}
\end{figure}

\subsection{Comparison with existing methods}

We compared \hyaline{} against several baseline approaches representing different methodological strategies (Table~\ref{tab:comparison}). The comparison reveals a clear hierarchy: methods that incorporate both evolutionary and geometric information substantially outperform those using either alone.

A random forest classifier trained on a curated set of 42 inter-residue distances (based on prior structural studies of activation) achieved \auroc{} of 0.891 on the temporal test set. This demonstrates that hand-crafted structural features capture substantial activation-relevant information---the distances between key residue pairs (e.g., TM3--TM6, TM3--TM7) do encode activation state. However, this approach requires expert knowledge to select relevant features, may miss important geometric relationships not captured by pairwise distances, and achieved lower performance than \hyaline{} despite careful feature engineering.

A sequence-only approach using ESM3 embeddings with a linear classifier achieved \auroc{} of 0.852, confirming that evolutionary information alone is insufficient. Despite the richness of ESM3 representations---which encode functional constraints shaped by hundreds of millions of years of evolution---they fundamentally cannot distinguish conformational states when the sequence is identical. A sequence CNN without evolutionary pretraining performed even worse (\auroc{} 0.789), highlighting the value of large-scale pretraining for extracting sequence-derived features.

Comparison with the recently published STAGS method\autocite{Caniceiro2025} provides additional context. STAGS uses random forests on inter-residue distance descriptors and reports 94.7\% accuracy on Class A receptors. While this represents strong performance, STAGS was developed and validated specifically for Class A receptors and may not generalize to other classes with different activation mechanisms. \hyaline{} achieves higher accuracy (97.6\% vs 94.7\%) while additionally providing robust classification across all four major \gpcr{} classes.

%-----------------------------------------------------------------------
% TABLE 2: Method Comparison
%-----------------------------------------------------------------------
\begin{table}[!htbp]
\centering
\caption{\textbf{Comparison with baseline methods.} Performance evaluated on the temporal test set (2023--2024 structures). \hyaline{} substantially outperforms approaches based on hand-crafted features or lacking geometric information.}
\label{tab:comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{\auroc} & \textbf{Accuracy (\%)} \\
\midrule
\hyaline{} (this work) & \textbf{0.991} & \textbf{96.4} \\
Random Forest + Distance Features & 0.891 & 86.3 \\
ESM3 + Linear Classifier & 0.852 & 81.7 \\
Sequence CNN & 0.789 & 76.2 \\
STAGS\textsuperscript{a} & 0.912 & 94.7 \\
Random & 0.500 & 50.0 \\
\bottomrule
\multicolumn{3}{l}{\footnotesize \textsuperscript{a}Reported on Class A receptors only; may not generalize to other classes.}
\end{tabular}
\end{table}

\subsection{Ablation studies quantify the value of geometric priors}

To understand the contribution of each architectural component, we performed systematic ablation studies where individual components were removed and models retrained using identical protocols (Fig.~\ref{fig:ablation}, Table~\ref{tab:ablation}). These ablations reveal that both evolutionary embeddings and geometric message passing are essential, with neither alone approaching the performance of the full model.

The ESM3 embeddings provided the largest single contribution: replacing them with one-hot amino acid encodings reduced \auroc{} from 0.995 to 0.823 on the cross-validation set, a decrease of 17.2 percentage points. This dramatic drop confirms the value of large-scale protein language model pretraining---ESM3's representations encode functional constraints that cannot be learned from the limited \gpcr{} training set alone. The evolutionary information complements the geometric features by providing context about which residues are functionally important and how they covary across receptor families.

Removal of the radial basis function distance encoding reduced \auroc{} to 0.971, demonstrating that explicit encoding of pairwise distances provides information beyond what is captured by the message passing operations alone. The RBF expansion allows the model to learn distance-dependent interaction patterns---for example, the characteristic 3.5~\AA{} distance of hydrogen bonds versus the 5--8~\AA{} range of salt bridges. Without this explicit distance encoding, the model must infer distance information purely from the message passing dynamics, which appears to be less efficient.

The motif attention biasing contributed modestly but significantly, with its removal reducing \auroc{} to 0.968. Importantly, analysis of attention patterns in the unbiased model revealed that it still learned to attend to the DRY, NPxxY, and CWxP motifs, albeit with slightly lower enrichment (2.4$\times$ vs 3.2$\times$ for DRY). This suggests that the biasing primarily accelerates learning by providing a useful inductive bias, rather than providing essential information unavailable from data. The model can discover the functional importance of these motifs purely from the training signal, but the biasing helps it do so more efficiently.

Finally, reducing the number of message passing layers from 5 to 3 decreased \auroc{} to 0.981. With a 10~\AA{} edge cutoff, each message passing layer propagates information approximately 10~\AA{} through the structure. Three layers provide a receptive field of roughly 30~\AA{}, which is marginally sufficient to span the DRY-NPxxY distance but may not fully integrate information from the extracellular and intracellular domains. Five layers (50~\AA{} receptive field) comfortably span the entire transmembrane domain. Increasing to 7 layers provided no additional benefit and slightly increased overfitting, suggesting that 5 layers represent an optimal trade-off.

%-----------------------------------------------------------------------
% FIGURE 5: Ablation Studies
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{4cm}
\textbf{FIGURE 5: Ablation Studies Quantify Component Contributions}\\[1em]
\textit{[Figure to be generated]}
\vspace{1cm}
}}
\caption{\textbf{Ablation studies quantifying component contributions.}
\textbf{a}, Bar chart showing \auroc{} for the full model and variants with individual components removed. ESM3 embeddings provide the largest contribution ($\Delta = -17.2\%$), followed by motif attention biasing ($\Delta = -2.7\%$), RBF distance encoding ($\Delta = -2.4\%$), and reduced layer count ($\Delta = -1.4\%$).
\textbf{b}, Effect of number of message passing layers on performance. Optimal performance is achieved with 5 layers (50~\AA{} receptive field); fewer layers limit information propagation across the transmembrane domain, while additional layers provide no benefit and increase overfitting risk.
\textbf{c}, Comparison of attention patterns with and without motif biasing. The unbiased model still learns to attend to activation motifs, but with lower enrichment, indicating that biasing accelerates rather than enables learning.
\textbf{d}, Training and validation loss curves for the full model, showing stable convergence without overfitting.}
\label{fig:ablation}
\end{figure}

%-----------------------------------------------------------------------
% TABLE 3: Ablation Results
%-----------------------------------------------------------------------
\begin{table}[!htbp]
\centering
\caption{\textbf{Ablation study results.} Performance impact of removing individual model components, evaluated by 5-fold cross-validation.}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Model Variant} & \textbf{\auroc} & $\boldsymbol{\Delta}$ \textbf{\auroc} & \textbf{Accuracy (\%)} \\
\midrule
Full Model & 0.995 & --- & 97.6 \\
$-$ ESM3 Embeddings & 0.823 & $-0.172$ & 78.4 \\
$-$ RBF Distance Encoding & 0.971 & $-0.024$ & 94.1 \\
$-$ Motif Attention Biasing & 0.968 & $-0.027$ & 93.8 \\
$-$ 2 Message Passing Layers & 0.981 & $-0.014$ & 95.2 \\
Random Baseline & 0.500 & $-0.495$ & 50.0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis of prediction confidence and challenging cases}

We analyzed the distribution of prediction confidence scores to identify patterns in model certainty and characterize challenging cases (Fig.~\ref{fig:case_studies}). The vast majority of predictions (94.3\%) had confidence scores above 0.9 or below 0.1, indicating high certainty. Intermediate confidence scores (0.3--0.7) were rare (2.8\% of predictions) and were associated with structures representing transitional or ambiguous conformational states.

Examination of the 39 misclassified structures revealed several patterns (Fig.~\ref{fig:case_studies}b). First, 15 misclassifications involved structures annotated as active based on bound agonist but lacking G protein or arrestin stabilization, which may represent partially activated or loosely coupled states. Second, 11 misclassifications involved class C receptors, consistent with their distinct activation mechanism. Third, 8 misclassifications involved structures with significant crystallographic artifacts or missing loop regions that may have compromised structural accuracy. These patterns suggest that most errors reflect genuine biological ambiguity or experimental limitations rather than model failures.

%-----------------------------------------------------------------------
% FIGURE 6: Case Studies
%-----------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{4cm}
\textbf{FIGURE 6: Analysis of Prediction Confidence and Challenging Cases}\\[1em]
\textit{[Figure to be generated]}
\vspace{1cm}
}}
\caption{\textbf{Analysis of prediction confidence and challenging cases.}
\textbf{a}, Distribution of prediction confidence scores. The bimodal distribution indicates high certainty for most predictions, with only 2.8\% of structures receiving intermediate confidence scores (0.3--0.7).
\textbf{b}, Categorization of the 39 misclassified structures by likely cause. Partially activated states (agonist-bound without transducer): $n=15$. Class C receptors with distinct mechanism: $n=11$. Structures with experimental artifacts: $n=8$. Unknown: $n=5$.
\textbf{c}, Example of a correctly classified challenging case: an allosteric modulator-bound structure (PDB: 7UM5) showing an intermediate conformation that was correctly assigned as inactive with confidence 0.72.
\textbf{d}, Example of an instructive misclassification: an agonist-bound structure (PDB: 8GY7) annotated as active but predicted as inactive (confidence 0.61). Structural analysis reveals that TM6 remains in a closed conformation despite agonist binding, suggesting a pre-activated intermediate state.}
\label{fig:case_studies}
\end{figure}

%=======================================================================
% DISCUSSION
%=======================================================================
\section{Discussion}

We have presented \hyaline{}, a geometric deep learning framework that achieves near-perfect accuracy in predicting \gpcr{} activation states from three-dimensional structures. By unifying evolutionary embeddings from protein language models with E(n)-equivariant graph neural networks, \hyaline{} bridges the sequence-structure gap that has limited previous computational approaches. The model's attention mechanisms provide interpretable insights into the structural basis of its predictions, validating that it has learned the underlying physics of receptor activation rather than superficial correlates.

The exceptional performance of \hyaline{} can be attributed to three key architectural choices. First, ESM3 embeddings provide rich representations of evolutionary and functional information that complement explicit structural features---the 17.2\% \auroc{} decrease without these embeddings underscores the value of large-scale protein language model pretraining for downstream structural tasks. Second, the E(n)-equivariant architecture provides an appropriate inductive bias for molecular structure modeling, ensuring that predictions depend only on the relative arrangement of atoms rather than arbitrary coordinate frames. This is a principled approach that outperforms the alternative of learning rotation invariance through data augmentation. Third, the soft motif attention biasing accelerates learning by incorporating prior biological knowledge while remaining flexible enough to discover additional relevant features from data.

A key strength of our validation approach is the rigorous temporal split. By training on structures deposited before 2023 and testing on 2023--2024 structures, we ensure that performance reflects genuine generalization rather than memorization. This is particularly important given concerns about data leakage and overfitting in machine learning for structural biology. The minimal performance gap between cross-validation and temporal test sets (0.995 vs 0.991 \auroc{}) provides confidence that \hyaline{} will maintain accuracy as new \gpcr{} structures are determined.

Several limitations of the current work should be noted. The class imbalance toward active structures (72.7\%) and the predominance of Class A receptors (79.4\%) may limit performance on underrepresented categories. While performance on Class C and Class F receptors remained strong, the confidence intervals were wider due to smaller sample sizes. The model is trained for binary classification and does not address the continuum of activation states or the distinction between G protein-biased and arrestin-biased conformations---important directions for future development. Additionally, \hyaline{} is trained on experimental structures and may not perform optimally on computationally predicted structures that contain systematic errors or represent non-physiological conformations.

\hyaline{} has immediate applications in several areas. For structure determination, the model can provide rapid annotation of activation state for newly determined structures, complementing manual inspection. For molecular dynamics simulations, \hyaline{} can classify conformational states along trajectories, enabling automated identification of activation events and analysis of transition pathways\autocite{Do2023,Plattner2015}. For structure-based drug discovery, classification confidence could help identify intermediate states that may be targeted by novel allosteric modulators. Perhaps most significantly, \hyaline{} could address a known limitation of AI-based structure prediction methods: assessing whether AlphaFold 3, Boltz-1, or Chai-1 models represent active or inactive conformations\autocite{Abramson2024,Wohlwend2024,ChaiDiscovery2024}. Recent benchmarks have shown that these methods frequently produce ambiguous or intermediate states, and \hyaline{} provides an orthogonal assessment of predicted conformational states.

In conclusion, \hyaline{} demonstrates that geometric deep learning can resolve the sequence-structure gap for conformational state prediction in \gpcr{}s. As the structural database continues to expand through cryo-EM and AI-based prediction, automated annotation tools will become increasingly important. The principles underlying \hyaline{}---unifying evolutionary and structural representations through equivariant architectures---are broadly applicable and may extend to conformational state prediction in other protein families.

%=======================================================================
% METHODS
%=======================================================================
\section{Methods}

\subsection{Dataset construction and preprocessing}

We retrieved all \gpcr{} structures from the Protein Data Bank (PDB) as of December 2024, cross-referenced with the GPCRdb database for receptor identification and activation state annotations. Structures were retained if they: (1) contained at least 80\% of the canonical transmembrane domain resolved; (2) had resolution $\leq$ 4.0 \AA{} for X-ray structures or FSC $\leq$ 4.5 \AA{} for cryo-EM structures; (3) had unambiguous activation state annotation in GPCRdb. Structures with significant missing loops ($>$30 consecutive unresolved residues) or obvious crystallographic artifacts were excluded after manual inspection.

For temporal validation, structures were divided based on PDB deposition date: training set (deposited before January 1, 2023; $n = 1,312$) and temporal test set (deposited January 2023 -- December 2024; $n = 278$). This split ensures that test structures were unavailable during model development.

Activation state labels were assigned based on GPCRdb annotations and ligand binding status. Active structures included: (1) G protein-coupled or G protein-mimetic nanobody-bound structures; (2) arrestin-coupled structures; (3) full agonist-bound structures without transducer but annotated as active based on conformational criteria. Inactive structures included: (1) apo structures; (2) antagonist-bound structures; (3) inverse agonist-bound structures. Structures with partial agonists or ambiguous annotations were excluded to ensure clean training labels.

For each structure, we extracted C$_\alpha$ coordinates and amino acid sequences using BioPython. Multi-chain structures were processed to retain only the receptor chain. Coordinates were centered at the geometric centroid of C$_\alpha$ atoms but not otherwise normalized, as the equivariant architecture is invariant to rotations and translations.

\subsection{ESM3 embedding extraction}

Protein language model embeddings were extracted using ESM3-Open (esm3-open-2024-03) with default parameters. For each receptor sequence, we obtained the mean of hidden representations across layers, yielding a 1,536-dimensional embedding per residue. Embeddings were precomputed and cached to accelerate training. For structures with missing residues in loops, we used the full UniProt sequence for embedding extraction and aligned embeddings to resolved coordinates using sequence alignment.

\subsection{Graph construction}

Each structure was represented as a graph $G = (V, E)$ where vertices correspond to residues and edges connect residue pairs with C$_\alpha$--C$_\alpha$ distance $\leq$ 10 \AA. Node features were initialized as the concatenation of ESM3 embeddings (1,536 dimensions) and sinusoidal positional encodings of sequence position (64 dimensions). Edge features were computed as radial basis function (RBF) expansions of pairwise distances:

\begin{equation}
\phi_k(d) = \exp\left(-\frac{(d - \mu_k)^2}{2\sigma^2}\right)
\end{equation}

where $d$ is the distance, $\mu_k$ are 96 uniformly spaced means from 2 to 20 \AA{}, and $\sigma = 0.3$ \AA{} is a learned width parameter initialized based on the mean spacing between distance bins.

\subsection{Motif detection and attention biasing}

Conserved activation motifs were identified using sequence pattern matching and structural criteria. The DRY motif was detected as the sequence D[R/K]Y in TM3 (Ballesteros-Weinstein positions 3.49--3.51). The NPxxY motif was detected as NP[A-Z][A-Z]Y in TM7 (positions 7.49--7.53). The CWxP motif was detected as C[W/F]xP in TM6 (positions 6.47--6.50). For class B1 and C receptors, modified patterns were used based on class-specific consensus sequences from GPCRdb.

Residues identified as belonging to motifs received an additive attention bias $b_{\text{motif}} = 0.5$ in the first attention layer, implemented as:

\begin{equation}
\alpha'_{ij} = \alpha_{ij} + b_{\text{motif}} \cdot \mathbf{1}_{\text{motif}}(i)
\end{equation}

where $\alpha_{ij}$ are the pre-softmax attention logits and $\mathbf{1}_{\text{motif}}(i)$ is an indicator for residue $i$ belonging to a conserved motif.

\subsection{E(n)-equivariant message passing}

We employed the EGNN architecture\autocite{Satorras2021} with modifications for our application. Each message passing layer updates both node features $h_i$ and coordinates $x_i$:

\begin{align}
m_{ij} &= \phi_e(h_i, h_j, ||x_i - x_j||^2, a_{ij}) \\
x_i' &= x_i + C \sum_{j \neq i} (x_i - x_j) \phi_x(m_{ij}) \\
m_i &= \sum_{j \neq i} m_{ij} \\
h_i' &= \phi_h(h_i, m_i)
\end{align}

where $\phi_e$, $\phi_x$, and $\phi_h$ are learned MLPs, $a_{ij}$ are edge attributes (RBF distances), and $C$ is a normalization constant. The coordinate update construction ensures equivariance to rotations and translations.

We used 5 message passing layers with hidden dimension 320 and ReLU activations. Layer normalization was applied after each message passing step. Dropout with probability 0.1 was applied to node features during training.

\subsection{Global attention pooling and classification}

After message passing, node features were aggregated into a graph-level representation using attention pooling:

\begin{align}
\alpha_i &= \frac{\exp(w^\top h_i)}{\sum_j \exp(w^\top h_j)} \\
h_G &= \sum_i \alpha_i h_i
\end{align}

where $w$ is a learned attention vector. The graph representation $h_G$ passed through a two-layer MLP (320 $\rightarrow$ 160 $\rightarrow$ 1) with ReLU activation, dropout (0.2), and a final sigmoid to produce the classification probability.

\subsection{Training procedure}

Models were trained using binary cross-entropy loss with the AdamW optimizer (learning rate $3 \times 10^{-4}$, weight decay $10^{-4}$). We employed a cosine annealing learning rate schedule with linear warmup over 5 epochs. Training proceeded for 30 epochs with early stopping based on validation loss (patience 5 epochs).

Data augmentation consisted of random coordinate noise (Gaussian, $\sigma = 0.1$ \AA) applied during training to improve robustness. No rotational augmentation was needed due to the equivariant architecture.

For cross-validation on the training set, structures were clustered at 30\% sequence identity using MMseqs2, and splits were performed at the cluster level to prevent data leakage. All reported cross-validation metrics are averages across 5 folds.

\subsection{Implementation and computational requirements}

\hyaline{} was implemented in PyTorch 2.1 with PyTorch Geometric for graph operations. ESM3 embeddings were computed using the official ESM implementation. Training was performed on a single NVIDIA A100 GPU (40GB) with typical training time of 4 hours for full cross-validation. Inference time is approximately 0.5 seconds per structure on GPU or 3 seconds on CPU.

Code and trained models are available at \url{https://github.com/Varosync/Hyaline}.

\subsection{Statistical analysis}

Confidence intervals for \auroc{} and other metrics were computed as the mean $\pm$ 1.96 standard errors across cross-validation folds, assuming approximate normality. For the temporal test set, confidence intervals were computed using 1,000 bootstrap resamples. Comparisons between model variants in ablation studies used paired $t$-tests on per-fold metrics. Significance of attention enrichment at motif positions was assessed using permutation tests with 10,000 permutations. All statistical analyses were performed in Python using scipy.stats.

%=======================================================================
% ACKNOWLEDGMENTS AND DECLARATIONS
%=======================================================================
\paragraph{Data Availability Statement}
All structures used in this study are publicly available from the Protein Data Bank. The curated dataset, trained models, and analysis code are available at \url{https://github.com/Varosync/Hyaline}.

\paragraph{Code Availability}
\hyaline{} is implemented in Python and freely available at \url{https://github.com/Varosync/Hyaline} under the MIT License.


\clearpage
\section*{Extended Data}

\subsection*{Extended Data Figure 1: Training Dynamics and Scalability}

\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{4cm}
\textbf{EXTENDED DATA FIGURE 1: Training Dynamics and Scalability}\\[1em]
\textit{[Figure to be generated]}
\vspace{1cm}
}}
\caption{\textbf{Training dynamics, convergence, and computational scalability.}
\textbf{a}, Training and validation loss curves over 30 epochs, showing stable convergence without overfitting. Final training loss: 0.065; final validation loss: 0.169.
\textbf{b}, Learning rate schedule showing linear warmup (5 epochs) followed by cosine annealing.
\textbf{c}, Per-fold performance across 5-fold cross-validation, demonstrating consistent results (\auroc{} range: 0.991--0.998).
\textbf{d}, Inference time as a function of receptor length, showing linear scaling. Mean inference time: 0.5 seconds per structure on GPU.}
\label{fig:ext_training}
\end{figure}

\subsection*{Extended Data Figure 2: Structural Basis of Activation}

\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{5cm}
\textbf{EXTENDED DATA FIGURE 2: Structural Comparison of Active and Inactive States}\\[1em]
\textit{[PyMOL renders to be generated]}
\vspace{1cm}
}}
\caption{\textbf{Structural basis of \gpcr{} activation detected by \hyaline.}
\textbf{a}, Side view of superimposed active (blue) and inactive (red) $\beta_2$-adrenergic receptor structures (PDB: 3SN6, 2RH1), showing the characteristic outward movement of TM6 in the active state. Arrow indicates the $\sim$14 \AA{} displacement of the intracellular end of TM6.
\textbf{b}, Cytoplasmic view highlighting the opening of the intracellular cavity in the active state that accommodates G protein binding.
\textbf{c}, Close-up of the DRY-NPxxY-CWxP motif triangle, showing the conformational changes in these conserved microswitches. Distances between key residue pairs are indicated.
\textbf{d}, Overlay showing TM6 conformations across multiple class A receptors in active (blue spectrum) and inactive (red spectrum) states, demonstrating the universality of the outward TM6 movement as the primary activation signature.}
\label{fig:ext_structures}
\end{figure}

\subsection*{Extended Data Figure 3: Class-Specific Activation Mechanisms}

\begin{figure}[!htbp]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{4cm}
\textbf{EXTENDED DATA FIGURE 3: Class-Specific Activation Mechanisms}\\[1em]
\textit{[Figure to be generated]}
\vspace{1cm}
}}
\caption{\textbf{Variation in activation mechanisms across \gpcr{} classes.}
\textbf{a}, Schematic diagrams of transmembrane helix movements upon activation for each class. Class A: TM6 outward rotation ($\sim$14 \AA{}), TM5 inward movement. Class B1: Pronounced TM6 kinking and extracellular unwinding. Class C: Minimal intramolecular changes; activation primarily involves dimer reorientation. Class F: Intermediate TM6 movement with additional TM5 and TM7 contributions.
\textbf{b}, Class-specific attention weight patterns learned by \hyaline{}, showing adaptation to different activation mechanisms.
\textbf{c}, Confusion matrices stratified by receptor class, revealing that most misclassifications occur in Class C receptors consistent with their distinct mechanism.}
\label{fig:ext_classes}
\end{figure}

\subsection*{Extended Data Table 1: Complete Performance Metrics by Receptor Family}

\begin{table}[!htbp]
\centering
\caption{\textbf{Complete performance metrics stratified by receptor family.}}
\label{tab:ext_family}
\begin{tabular}{lcccccc}
\toprule
\textbf{Class} & $\boldsymbol{n}$ & \textbf{\auroc} & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{Specificity} & \textbf{F1} \\
\midrule
Class A & 1,263 & 0.997 & 98.3\% & 99.2\% & 95.1\% & 0.988 \\
Class B1 & 145 & 0.988 & 95.9\% & 97.1\% & 91.3\% & 0.968 \\
Class C & 94 & 0.971 & 91.5\% & 93.2\% & 87.5\% & 0.938 \\
Class F & 34 & 0.956 & 88.2\% & 90.0\% & 85.7\% & 0.900 \\
\midrule
Overall & 1,590 & 0.995 & 97.6\% & 98.96\% & 93.78\% & 0.983 \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{Extended Data Table 2: Hyperparameter Configuration}

\begin{table}[!htbp]
\centering
\caption{\textbf{Model hyperparameters and training configuration.}}
\label{tab:ext_hyperparams}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{l}{\textit{Model Architecture}} \\
ESM3 embedding dimension & 1,536 \\
Hidden dimension & 320 \\
Number of message passing layers & 5 \\
Attention heads & 8 \\
RBF centers & 96 (2--20 \AA) \\
Edge cutoff distance & 10 \AA \\
Motif attention bias & 0.5 \\
\midrule
\multicolumn{2}{l}{\textit{Training}} \\
Optimizer & AdamW \\
Learning rate & $3 \times 10^{-4}$ \\
Weight decay & $10^{-4}$ \\
Batch size & 32 \\
Epochs & 30 \\
Warmup epochs & 5 \\
Dropout (features) & 0.1 \\
Dropout (classifier) & 0.2 \\
Coordinate noise ($\sigma$) & 0.1 \AA \\
\midrule
\multicolumn{2}{l}{\textit{Validation}} \\
Cross-validation folds & 5 \\
Sequence identity threshold & 30\% \\
Temporal split cutoff & January 1, 2023 \\
\bottomrule
\end{tabular}
\end{table}

%=======================================================================
% BIBLIOGRAPHY
%=======================================================================
\printendnotes

\defbibnote{preamble}{References}
\printbibliography[prenote={preamble}]

\end{document}
